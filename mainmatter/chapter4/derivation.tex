\section{Derivation of the \glsfmtlong{js} fold change estimator}
\label{sec:JS_derivation}

This section will define variables and symbols as they arise.
For a complete list of conventions and notation used in this section, see \Cref{sec:JS_notation}.

For a $p$-variate normal distribution with mean $\mu$ and covariance matrix equal to the identify matrix, denoted as $\mathcal{N}_p(\mu, I_p)$, if the mean is unknown, the following theorem holds \cite{steinInadmissibilityUsualEstimator1956}:

\begin{theorem}
  Consider the distribution $\mathcal{N}_p \left( \mu, I_p \right)$ with unknown mean, $\mu$, from which a single sample, $Z$, is drawn.
  Consider an estimator for the mean, $\muols$, that is equal to the single observation (i.e. $\muols = Z$).
  The estimator $\muols$ does not minimize the \gls{mse}, $\expect{(\mu - \muols)^2}$ over all possible estimators.
  Namely, the estimator $\mujs = \left( 1 - \frac{b}{a + \Vert Z \Vert ^2}\right) Z$ has a smaller \gls{mse} than $\muols$ for a sufficiently small coefficient $b$ and sufficiently large coefficient $a$.
\end{theorem}

This result was generalized to non-singular covariance matrices that were not necessarily the identity matrix \cite{jamesEstimationQuadraticLoss1961,bockMinimaxEstimatorsMean1975}:

\begin{theorem}
  Consider the distribution $\mathcal{N}_p \left(\mu, \Sigma \right)$ where the mean, $\mu$, is unknown and the covariance matrix, $\Sigma$, is known.
  Consider a single sample, $Z$, from this distribution and let $\mujs = \left( 1 - \frac{c}{Z^\tran \Sigma^{-1} Z}\right) Z$ where $c$ is some scaling coefficient be an estimator for $\mu$.
  If $p \ge 3$, $\trace{\Sigma} \ge 2 \lambda$ (where $\trace{\cdot}$ is the trace of a matrix, $\lambda$ is the largest eigenvalue of the covariance matrix, $\Sigma$), and $0 \le c \le 2 \left( \frac{\trace{\Sigma}}{\lambda} - 2 \right)$, then $\mujs$ is the estimator that minimizes the \gls{mse}.
  \label{thm:js}
\end{theorem}

We can demonstrate how the Sleuth statistical model for performing differential expression analysis \cite{pimentelDifferentialAnalysisRNAseq2017,yiGenelevelDifferentialAnalysis2018} can be transformed to fit the criteria for \Cref{thm:js}.

Consider an experiment with $n_\mathrm{WT}$ \gls{wt} samples and 1 mutant sample where \gls{rnaseq} is performed.
Sequencing reads from each sample are mapped to each transcript in the organism's transcriptome, $S$.
For each of the $n_\mathrm{WT}$ \gls{wt} samples, \gls{rnaseq} read counts can be modelled as:
%
\begin{equation}
  D_s \sim \mathcal{N} \left( \beta_{0s}, \sigma_s^2 + \tau_s^2 \right)
  \label{eqn:wt_model}
\end{equation}
%
where $D_s$ is the abundance of transcript $s \in S$ and $\beta_{0s}$ is the mean abundance for transcript $s$.
The variance in this model is decomposed into two components: biological noise arising from differences between biological replicates and library preparation, and technical noise arising from the stochastic nature of sequencing measurements and computational analysis of sequecing reads \cite{pimentelDifferentialAnalysisRNAseq2017}.
$\sigma_s^2$ denotes the biological variance of counts for transcript $s$ and $\tau_s^2$ denotes the inferential variance for transcript $s$.
For details about this variance decomposition, see \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017} and \Cref{chap:JS_appendix}.

The model for the single mutated sample is slightly modified, with a parameter $\beta_{s,1}$ representing the fold change effect of the mutation:
%
\begin{equation}
  D_s \sim \mathcal{N} \left( \beta_{0s} + \beta_{1s}, \sigma_s^2 + \tau_s^2 \right)
  \label{eqn:single_mut_model}
\end{equation}
%
Under this model it is assumed that both the biological and inferential variances are the same between the mutated and \gls{wt} samples.
All of the parameters in this model are unknown and will be estimated from the data.
While $\beta_{0s}$ can be estimated from the \gls{wt} samples, the fold change $\beta_{1s}$ is can only be estimated from the single mutated sample.
By reparameterizing this model to consider every transcript, \Cref{eqn:wt_model} can be re-written as an $|S|$-dimensional random vector:
%
\begin{equation}
  \Delta \sim \mathcal{N}_{|S|}(\Beta_0, \Sigma) \\
  \label{eqn:delta_wt}
\end{equation}
%
and \Cref{eqn:single_mut_model} can be similarly re-written as:
%
\begin{equation}
  \nabla \sim \mathcal{N}_{|S|}(\Beta_0 + \Beta_1, \Sigma) \\
  \label{eqn:delta_mut}
\end{equation}
%
where $\Delta$ is the vector of abundances for all transcripts in a \gls{wt} sample; $\nabla$ is the same but for the mutated sample; $\Beta_0$ is the $|S|$-dimensional vector of baseline abundances; $\Beta_1$ is the $|S|$-dimensional vector of fold change effects associated with the mutation; and $\Sigma$ is the covariance matrix.
Mathematically, we can express these parameters in terms of parameters from \Cref{eqn:single_mut_model}:
%
\begin{align*}
  \Beta_{0s} & = \beta_{0s} \forall s \in S \\
  \Beta_{1s} & = \beta_{1s} \forall s \in S \\
  \Sigma     & = \begin{bmatrix}
    \sigma_1^2 + \tau_1^2 &        & 0                             \\
                          & \ddots                                 \\
    0                     &        & \sigma_{|S|}^2 + \tau_{|S|}^2 \\
  \end{bmatrix}
\end{align*}

The sleuth statistical model can be used derive estimates for $\Beta_0$ and $\Sigma$ solely from the $n_\mathrm{WT}$ \gls{wt} samples (hereafter denoted with the $\hat{}$ symbol).
If we treat the estimates $\hat{\Beta}_0$ and $\hat{\Sigma}$ derived from the \gls{wt} samples as plug-in parameters for the mutated model, then \Cref{eqn:single_mut_model} for a single mutated sample meets the criteria for the \gls{js} estimators.
This is not necessarily an accurate assumption, since these estimates may be biased, inaccurate at small samples sizes, or affected by confounding factors, like batch effects, that are not shared between the \gls{wt} and mutant samples.
With sufficiently large $n_\mathrm{WT}$, this may not be a practical concern.

Under these assumptions, we have the following distribution from which we are drawing a single observation:
%
\begin{equation}
  \nabla - \hat{\Beta}_0 \sim \mathcal{N}_{|S|} \left( \Beta_1, \hat{\Sigma} \right)
\end{equation}
%
This fits the criteria for \Cref{thm:js} and a \gls{js} estimator for the unknown fold change, $\Beta_1$, can be constructed:
%
\begin{equation}
  \fcjs = \left( 1 - \frac{c}{(\nabla - \hat{\Beta}_0)^\tran \hat{\Sigma}^{-1} (\nabla - \hat{\Beta}_0)} \right) \left(\nabla - \hat{\Beta}_0 \right)
  \label{eqn:js_defn}
\end{equation}

\subsection{Comparison between the \glsfmtshort{ols} and \glsfmtshort{js} estimators}

To demonstrate how the \gls{js} estimator, $\fcjs$, compares to conventional approaches for estimating gene expression fold change that do not make use of \Cref{thm:js}, we can consider the \gls{ols} estimator.
For the experimental design described above, the \gls{ols} estimator for $\Beta_1$, $\fcols$, is given by:
%
\begin{equation}
  \fcols = \nabla - \hat{\Beta}_0
  \label{eqn:ols_defn}
\end{equation}
%
Substituting this into \Cref{eqn:js_defn} yields a simplified form of the \gls{js} estimator for $\Beta_1$:
%
\begin{equation}
  \fcjs = \left( 1 - \frac{c}{\left( \fcols \right)^\tran \hat{\Sigma}^{-1} \fcols} \right) \fcols
  \label{eqn:js_defn_ols}
\end{equation}
%
From this definition, one can see that the \gls{js} estimate is colinear with the \gls{ols} estimate but uniformly shrunk towards 0.
We can summarize the above with the following theorem.

\begin{theorem}
  For an experiment containing $n_\mathrm{WT}$ \gls{wt} samples and a single mutated sample, an estimate for the expression fold change of $|S|$ transcripts can be given by the \gls{js} estimator \Cref{eqn:js_defn_ols} where $\hat{\Sigma}$ is the covariance matrix for all $|S|$ transcripts estimated from all $n_\mathrm{WT}$ \gls{wt} samples; $\fcols$ is the \gls{ols} estimator for expression fold change given by \Cref{eqn:ols_defn}; and $c$ is some scaling coefficient.
  When $|S| \ge 3$, $\trace{\hat{\Sigma}} \ge 2\lambda$ (where $\lambda$ is the largest eigenvalue of $\hat{\Sigma}$), and $c$ satisfies \Cref{eqn:js_scaling_bounds}, then the \gls{mse} of the \gls{js} estimator, $\fcjs$, is smaller than the \gls{mse} of the \gls{ols} estimator, $\fcols$.
\end{theorem}

It can be shown that the \gls{js} estimator is biased towards 0 with a smaller variance than the \gls{ols} estimator (see \Cref{sec:JS_moments}).
In theory, this may increase the error of some transcripts, but will decrease the \gls{mse} for a set of transcripts in aggregate (see \Cref{sec:JS_moments}).

There are two parameters of this model that will affect the amount of biasing: the scaling coefficient, $c$, and the transcripts being considered for differential expression, $S$.
Firstly, the scaling coefficient can be manually specified, and the largest biasing occurs when $c$ is its maximum value, $2 \left( \frac{\trace{\Sigma}}{\lambda} - 2 \right)$.
Secondly, the transcripts under consideration can also be manually specified, which will affect the value of the denominator, $\left( \fcols \right)^\tran \hat{\Sigma}^{-1} \fcols$, and thus the amount of biasing.
The more transcripts under consideration, the larger the denominator, and so the smaller the effect.
Taken together, we have produced a high-bias, low-variance fold change estimator that has a lower \gls{mse} than the \gls{ols} estimator and two tunable parameters.
