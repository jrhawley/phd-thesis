\section{Results}

\subsection{Derivation of the \glsentrylong{js} fold change estimator}
\label{sec:JS_derivation}

For a $p$-variate normal distribution $Z \sim \mathcal{N}_p(\mu, \Sigma)$ where $\mu$ is unknown and $\Sigma$ is known, the following theorem holds \cite{steinInadmissibilityUsualEstimator1956}:

\begin{theorem}
  The estimator $\hat{\mu}^{(0)} = Z$, for any mean $\mu$, does not minimize the \gls{mse} $\mathbb{E} \left[ (\mu - \hat{\mu})^2 \right]$ for a single observation of $Z$ when $p \ge 3$ and $\Sigma = I_p$. Namely, the estimator $\hat{\mu}^{(JS)} = \left( 1 - \frac{b}{a + \Vert Z \Vert ^2}\right) Z$ has a smaller \gls{mse} than $\hat{\mu}^{(0)}$ for a single observation for sufficiently small $b$ and large $a$.
\end{theorem}

This result was generalized to non-singular covariance matrices that were not necessarily the identity matrix (Theorem 2 of \cite[REF][]{bockMinimaxEstimatorsMean1975}):

\begin{theorem}
  Let $Z \sim \mathcal{N}_p \left(\mu, \Sigma \right)$.
  Let $\hat{\mu}^{(JS)} = \left( 1 - \frac{c}{Z^T \Sigma^{-1} Z}\right) Z$.
  If $p \ge 3$, $\text{Tr}(\Sigma) \ge 2 \lambda_L$ (where $\lambda_L$ is the largest eigenvalue of the covariance matrix, $\Sigma$), and $0 \le c \le 2 \left( \frac{\text{Tr}(\Sigma)}{\lambda_L} - 2 \right)$, then $\hat{\mu}^{(JS)}$ is the estimator for the mean, $\mu$, that minimizes the \gls{mse} for a single observation of $Z$.
\end{theorem}

Consider the differential analysis model used in the Sleuth R package \cite{pimentelDifferentialAnalysisRNAseq2017, yiGenelevelDifferentialAnalysis2018} for a single transcript, $s$, with the simple experimental design of $n_{wt}$ \gls{wt} samples and 1 mutant sample:

\begin{equation*}
  D_s | Y_s \sim \mathcal{N}_N \left( \beta_{s,0} + \mathbb{I}_{mut}\beta_{s,1}, (\sigma_s^2 + \tau_s^2)I_N \right)
\end{equation*}

For the $n_{wt}$ \gls{wt} samples, this is equivalent to

\begin{equation*}
  D_s | Y_s \sim \mathcal{N}_{n_{wt}} \left( \beta_{s,0}, (\sigma_s^2 + \tau_s^2)I_{n_{wt}} \right)
\end{equation*}

which can be fit with the same model process that Sleuth employs.
For the single mutated sample, this model is

\begin{equation}
  D_s | Y_s \sim \mathcal{N} \left( \beta_{s, 0} + \beta_{s, 1}, \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right)
  \label{eqn:single_mut_model}
\end{equation}

The covariance matrix is the same as the mutated samples, but the mean $\beta_{s, 0} + \beta_{s, 1}$ is unknown.
By reparameterizing this model to consider every transcript in the single mutated sample, \Cref{eqn:single_mut_model} can be re-written as follows:

\begin{equation}
  \Delta \sim \mathcal{N}_{|S|}(\Beta_0 + \Beta_1, \Sigma) \\
\end{equation}

where

\begin{align*}
  \Beta_{i,s} & = \beta_{s,i} \forall s \in S \\
  \Sigma      & = \begin{bmatrix}
    \max\{ \hat{\sigma}_1^2, \tilde{\sigma}_1^2 \} + \hat{\tau}_1^2 &        & 0                                                                           \\
                                                                    & \ddots &                                                                             \\
    0                                                               &        & \max\{ \hat{\sigma}_{|S|}^2, \tilde{\sigma}_{|S|}^2 \} + \hat{\tau}_{|S|}^2 \\
  \end{bmatrix}
\end{align*}

We switch from using coefficients $\beta_{t,i}$ to $\Beta_{i,s}$ to avoid confusion, since $\beta_{t,i} \in \mathbb{R}^p$ (a $p$-dimensional vector for each covariate in the design) whereas $\Beta_{i,s} \in \mathbb{R}^{|S|}$ (an $|S|$-dimensional vector for only a single coefficient over all transcripts in $S$).

Observations of a single mutated sample from this model meet the criteria for the \gls{js} estimators.
A \gls{js} estimator for the unknown fold change, $\Beta_1$, can be constructed.

\begin{equation}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{(\Delta - \hat{\Beta}_0)^T \Sigma^{-1} (\Delta - \hat{\Beta}_0)} \right)(\Delta - \hat{\Beta}_0)
  \label{eqn:js_defn}
\end{equation}

where $\hat{\Beta}_0$ is the estimate obtained from the non-mutated samples for all transcripts $s \in S$.

It is straightforward to see that $\text{Tr}(\Sigma) = \sum_{s \in S} \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2$ and that $\lambda_L = \max_{s \in S} \left\{ \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right\}$.

\subsection{Comparison between the \glsentryshort{ols} and \glsentrylong{js} estimators}

For a simple experimental design where the mutation status is the only coefficient the \gls{ols} estimator is given by:

\begin{equation*}
  \begin{bmatrix}
    \hat{\beta}_{s,0}^{(OLS)} \\
    \hat{\beta}_{s,1}^{(OLS)}
  \end{bmatrix}
  = \hat{\beta}_s^{(OLS)}
  = (X^TX)^{-1}X^T d_s
  = \begin{bmatrix}
    \bar{d}_s^{(wt)} \\
    d_s^{(mut)} - \bar{d}_s^{(wt)}
  \end{bmatrix}
\end{equation*}

where

\begin{equation*}
  X = \begin{bmatrix}
    1      & 1      \\
    1      & 0      \\
    \vdots & \vdots \\
    1      & 0
  \end{bmatrix}
  \in \mathbb{R}^{(N + 1) \times 2}
\end{equation*}

is the design matrix.
Looking closely at the \gls{ols} estimator for the mutation coefficient, $\beta_{s,1}$, it is clear that it is given by:

\begin{equation}
  \hat{\beta}_{s,1}^{(OLS)} = d_s^{(mut)} - \hat{\beta}_{s,0}^{(OLS)} = \delta_s - \hat{\beta}_{0,s}
\end{equation}

which is used directly in the definition of the \gls{js} estimator in \Cref{eqn:js_defn}.
The \gls{js} estimator for $\Beta_1$ can then be expressed simply as:

\begin{equation}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_1^{(OLS)} \right)^T \Sigma^{-1} \hat{\Beta}_1^{(OLS)}} \right) \hat{\Beta}_1^{(OLS)}
  \label{eqn:js_defn_ols}
\end{equation}

From this definition, it is easy to see that the \gls{js} estimate is colinear with the \gls{ols} estimate but uniformly shrunk towards 0.
For a more general experimental design, the above can be extended.

\begin{theorem}
  Given an experimental design matrix $X \in \mathbb{R}^{n \times p}$ where $n > p$, $\text{rank}(X) = p$ and $\text{rank}(X^*) = p - 1$ where $X^* \in \mathbb{R}^{(n - 1) \times p}$ is the same design matrix but with one sample removed, a \gls{js} estimator for the linear coefficient uniquely specified by the one sample is given by

  \begin{equation*}
    \hat{\Beta}_i^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_i^{(OLS)} \right)^T \Sigma^{-1} \hat{\Beta}_i^{(OLS)}} \right) \hat{\Beta}_i^{(OLS)}
  \end{equation*}
\end{theorem}

It can be shown that the \gls{js} estimator is biased towards 0 with a smaller variance than the \gls{ols} estimator (see \Cref{sec:JS_moments}).
In theory, this may increase the error of some transcripts, but will decrease the \gls{mse} for a set of transcripts in aggregate (see \Cref{sec:JS_moments}).

From this definition, we can see two parameters of this model that will affect the amount of biasing: the scaling coefficient, $c$, and the total number of transcripts being considered, $|S|$.
Firstly, the scaling coefficient can be manually specified, and the largest biasing occurs when $c$ is its maximum value, $2 \left( \frac{\text{Tr}(\Sigma)}{\lambda_L} - 2 \right)$.
Secondly, the transcripts under consideration can also be manually specified, which will affect the value of the denominator $\left( \hat{\Beta}_1^{(OLS)} \right)^T \Sigma^{-1} \hat{\Beta}_1^{(OLS)}$, and thus the amount of biasing.
The more transcripts under consideration, the larger the denominator, and thus the smaller the effect, compared to the \gls{ols} method.
Thus, we have produced a high-bias, low-variance fold change estimator that has a lower \gls{mse} than the \gls{ols} estimator and two tunable parameters.

\subsection{Empirical analysis of the \glsentrylong{js} estimator}

Now that the statistical properties of the \gls{js} estimator have been shown, we use empirical \gls{rnaseq} data to measure the performance of this method in practice.
To demonstrate this, we make use of a highly replicated \gls{rnaseq} experiment involving $\Delta$ \emph{Snf2} \gls{ko} and \gls{wt} yeast cells \cite{gierlinskiStatisticalModelsRNAseq2015}.
This dataset contains 48 biological replicates of each condition, an infeasible sample size for most \gls{rnaseq} experiments.
Experiments with small sample sizes can be compared to the full dataset to estimate the number of true and false detections for a given experimental design and a given method.
We randomly select $N$ total samples from the full dataset with an optimal even split between the two groups (i.e. $N / 2$ $\Delta$ \emph{Snf2} and $N / 2$ \gls{wt}) or a suboptimal $N - 1$-vs-1 split (i.e. $N - 1$ $\Delta$ \emph{Snf2} and 1 \gls{wt} or 1 $\Delta$ \emph{Snf2} and $N - 1$ \gls{wt}).
To measure the effect of total sample size, these simulations are repeated for multiple values of $N$.

We find that for all values of $N$, the \gls{js} method produces more \gls{tp} and \gls{fp} calls, as well as fewer \gls{tn} and \gls{fn} calls, than the \gls{ols} method with suboptimal designs, on average (two-way \gls{anova}, $p < 2.2 \times 10^{-16}$; \Cref{fig:JS_fig2}).
For example, for $N = 6$, the \gls{js} method identified 642.87 \gls{tp}, 58 \gls{fp}, 2098.3 \gls{tn}, and 2995.7 \gls{fn} calls on average, compared the to 520.7 \gls{tp}, 41.37 \gls{fp}, 2114.97 \gls{tn}, and 3117.93 \gls{fn} calls for the \gls{ols} method (23.5 \% more \gls{tp}, 40 \% more \gls{fp}, 1.8 \% fewer \gls{tp} and 3.9 \% fewer \gls{fn} calls; \Cref{fig:JS_fig2}).
The strength of this effect appears to decrease as the total number of samples increases.
Notably, for the $N = 4$ case where a suboptimal design would be most common in practice, the \gls{js} method had more \gls{tp} and fewer \gls{fn} than the optimal experimental design.
In all other cases, however, the optimal even split between $\Delta $ \emph{Snf2} and \gls{wt} groups results in the most \gls{tp} and fewest \gls{fn} calls, as expected.
Thus, for differential expression hypothesis testing, the \gls{ols} method can identify more \gls{tp} and fewer \gls{fn} calls than the \gls{ols} method when dealing with suboptimal experimental designs.

\newfigure{chapter4/Figure2.png}{Differential gene expression analysis of the entire yeast transcriptome with differently sized experimental designs}{Simulations ($n = 30$) using randomly selected samples which were then compared to the full dataset of 48 $\Delta$ \emph{Snf2} vs 48 \gls{wt} to calculate \gls{tp} (\textbf{a}), \gls{fp} (\textbf{b}), \gls{tn} (\textbf{c}), and \gls{fn} (\textbf{d}).}{fig:JS_fig2}

To investigate where the changes in statistical inferences come from, we can view a representative simulation (\Cref{fig:JS_fig3}).
In the full dataset with 48 biological replicates, \emph{\emph{Snf2}}, \emph{Snf2} is the most under-expressed gene with an estimated 99 \% reduction in expression (\Cref{fig:JS_fig3}a).
Three other example genes, \emph{PHO12}, \emph{TIS11}, and \emph{TYE7}, are also under-expressed in the $\Delta$ \emph{Snf2} cells.
Using 4 samples in total, evenly split between the two groups, all four genes remain detected as differentially expressed (\Cref{fig:JS_fig3}b).
Using a suboptimal design with 1 $\Delta$ \emph{Snf2} and 3 \gls{wt} samples, \emph{TIS11} and \emph{TYE7} are no longer detected as differentially expressed using the \gls{ols} method (\Cref{fig:JS_fig3}c).
However, the \emph{TIS11} gene is detected as differentially expressed using the \gls{js} method (\Cref{fig:JS_fig3}d).
The fold change estimates in \Cref{fig:JS_fig3}c-d are similar, since the biasing is small effect.
Thus, the differences in statistical inference result from the smaller variance of the \gls{js} estimator, as predicted.

\newfigure{chapter4/Figure3.png}{Differential gene expression analysis of $\Delta$ \emph{Snf2} vs \gls{wt} yeast cells using different sample sizes and experimental designs}{\textbf{a.} Volcano plot of differential expression results with \gls{ols} estimates in a highly replicated experiment consisting of 48 biological replicates of each condition. \textbf{b.} The same analysis as (\textbf{a}) using 4 samples in total, 2 $\Delta$ \emph{Snf2} and 2 \gls{wt} samples. \textbf{c.} The same analysis as (\textbf{c}) using 1 $\Delta$ \emph{Snf2} and 3 \gls{wt}. \textbf{d.} The same analysis as (\textbf{c}) using the \gls{js} method instead of \gls{ols}.}{fig:JS_fig3}

Finally, we investigated the impact of the number of transcripts considered on \gls{mse} reduction.
Using the same yeast \gls{rnaseq} data, we randomly selected $N = 6$ samples and a small number of transcripts from the entire transcriptome, then performed differential expression analysis with the \gls{ols} and \gls{js} methods.
This was repeated 30 times (15 simulations of 5 $\Delta$ \emph{Snf2} and 1 \gls{wt} and 15 simulations of 1 $\Delta$ \emph{Snf2} and 5 \gls{wt}) with a total of $|S| \in \{ 3, 10, 25, 50, 100, 250 \}$ transcripts.
Nearly all simulations show a smaller \gls{mse} when using the \gls{js} method compared to the \gls{ols} method (dots below the dotted lines in \Cref{fig:JS_fig4}a).
Moreover, the \gls{mse} is reduced by 7.73 - 22.68 \% using the \gls{js} method, on average, regardless of the number of transcripts considered.
The largest percentage of \gls{mse} reduction is observed when 3 transcripts are chosen, with an \textapprox 86 \% error reduction (\Cref{fig:JS_fig4}b).
However, the effect is also more variable when fewer transcripts are considered, as some simulations with 3 transcripts resulted in an \textapprox 150 \% increase in error (\Cref{fig:JS_fig4}b).
Taken together, we find that the \gls{js} method significantly reduces the fold change \gls{mse}, with greater reductions found by considering a smaller number of transcripts.

\newfigure{chapter4/Figure4.png}{Differential gene expression analysis focusing on a subset of trancsripts, not the entire transcriptome}{All experiments use 1 $\Delta$ \emph{Snf2} vs 5 \gls{wt} samples (or vice versa). \textbf{a.} Comparison of the \gls{mse} of the \gls{js} estimates ($y$-axis) against the \gls{ols} estimates ($x$-axis). The total number of transcripts in each comparison is specified above each facet. \textbf{b.} Percent different in \gls{mse} between the \gls{js} and \gls{ols} estimates. One-sided, two-sample paired Student's $t$-test, $n = 30$, FDR multiple test corrections.}{fig:JS_fig4}
