\section{Results}

\subsection{Derivation of the \glsentrylong{js} fold change estimator}
\label{sec:JS_derivation}

For a $p$-variate normal distribution $Z \sim \mathcal{N}_p(\mu, \Sigma)$ where $\mu$ is unknown and $\Sigma$ is known, the following theorem holds \cite{steinInadmissibilityUsualEstimator1956}:

\begin{theorem}
  The estimator $\hat{\mu}^{(0)} = Z$, for any mean $\mu$, does not minimize the \gls{mse} $\mathbb{E} \left[ (\mu - \hat{\mu}^{(0)})^2 \right]$ for a single observation of $Z$ when $p \ge 3$ and $\Sigma = I_p$, the $p \times p$ identity matrix.
  Namely, the estimator $\hat{\mu}^{(JS)} = \left( 1 - \frac{b}{a + \Vert Z \Vert ^2}\right) Z$ has a smaller \gls{mse} than $\hat{\mu}^{(0)}$ for a single observation for sufficiently small coefficient $b$ and sufficiently large coefficient $a$.
\end{theorem}

This result was generalized to non-singular covariance matrices that were not necessarily the identity matrix (Theorem 2 of \cite[REF][]{bockMinimaxEstimatorsMean1975}):

\begin{theorem}
  Let $Z \sim \mathcal{N}_p \left(\mu, \Sigma \right)$.
  Let $\hat{\mu}^{(JS)} = \left( 1 - \frac{c}{Z^\tran \Sigma^{-1} Z}\right) Z$.
  If $p \ge 3$, $\trace{\Sigma} \ge 2 \lambda$ (where $\trace{\cdot}$ is the trace of a matrix, $\lambda$ is the largest eigenvalue of the covariance matrix, $\Sigma$), and $0 \le c \le 2 \left( \frac{\trace{\Sigma}}{\lambda} - 2 \right)$, then $\hat{\mu}^{(JS)}$ is the estimator for the mean, $\mu$, that minimizes the \gls{mse} for a single observation of $Z$.
  \label{thm:js}
\end{theorem}

Consider the differential analysis model used in the Sleuth R package \cite{pimentelDifferentialAnalysisRNAseq2017,yiGenelevelDifferentialAnalysis2018} for a single transcript, $s$, with the simple experimental design of $n_\mathrm{WT}$ \gls{wt} samples and 1 mutant sample.
For the $n_\mathrm{WT}$ \gls{wt} samples, this can be expressed as:
%
\begin{equation}
  D_s \sim \mathcal{N}_{n_\mathrm{WT}} \left( \beta_{s,0}, (\sigma_s^2 + \tau_s^2)I_{n_\mathrm{WT}} \right)
  \label{eqn:wt_model}
\end{equation}
%
where $D_s$ is the $n_\mathrm{WT}$-dimensional count of reads compatible with transcript $s$ (i.e. the read counts mapping to transcript $s$ after pseudo-alignment); $\beta_{s,0}$ is the mean read count for transcript $s$; $\sigma_s^2$ is the biological variance of counts for transcript $s$; $\tau_s^2$ is the inferential variance for transcript $s$; and $I_{n_\mathrm{WT}}$ is the ${n_\mathrm{WT}} \times {n_\mathrm{WT}}$ identity matrix.
The model for the single mutated sample is slightly modified, with a parameter $\beta_{s,1}$ representing the fold-change effect of the mutation:
%
\begin{equation}
  D_s \sim \mathcal{N} \left( \beta_{s, 0} + \beta_{s, 1}, \sigma_s^2 + \tau_s^2 \right)
  \label{eqn:single_mut_model}
\end{equation}
%
For a given transcript, $s$, the variance is the same between the mutated and \gls{wt} samples (namely, $\sigma_s^2 + \tau_s^2$).
While the mean $\beta_{s, 0}$ can be estimated from the \gls{wt} samples, the fold-change $\beta_{s, 1}$ is unknown and can only be determined through the single mutated sample.
By re-parameterizing this model to consider every transcript, \Cref{eqn:wt_model} can be re-written as:
%
\begin{equation}
  \Delta^{(\mathrm{WT})} \sim \mathcal{N}_{|S|}(\Beta_0, \Sigma) \\
  \label{eqn:delta_wt}
\end{equation}
%
and \Cref{eqn:single_mut_model} can be re-written as:
%
\begin{equation}
  \Delta^{(\mathrm{Mut})} \sim \mathcal{N}_{|S|}(\Beta_0 + \Beta_1, \Sigma) \\
  \label{eqn:delta_mut}
\end{equation}
%
where $\Delta^{(\mathrm{WT})}$ is the $|S|$-dimensional vector of read counts for all transcripts in a \gls{wt} sample; $\Delta^{(\mathrm{Mut})}$ is the same but for the mutated sample; $S$ is the set of all transcripts; $\Beta_0$ is the $|S|$-dimensional vector of mean counts for each transcript; $\Beta_1$ is the $|S|$-dimensional vector of fold-change effects associated with the mutation; and $\Sigma$ is the covariance matrix.
Mathematically, we can express these parameters in terms of parameters from \Cref{eqn:single_mut_model}:
%
\begin{align*}
  \Beta_{0,s} & = \beta_{s,0} \forall s \in S \\
  \Beta_{1,s} & = \beta_{s,1} \forall s \in S \\
  \Sigma      & = \begin{bmatrix}
    \sigma_1^2 + \tau_1^2 &        & 0                             \\
                          & \ddots                                 \\
    0                     &        & \sigma_{|S|}^2 + \tau_{|S|}^2 \\
  \end{bmatrix}
\end{align*}

All quantities in $\Beta_0$, $\Beta_1$, and $\Sigma$ are initially unknown.
The statistical model presented in \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017} can be used derive estimates for $\Beta_0$ and $\Sigma$ solely from the \gls{wt} samples (hereafter denoted with the $\hat{}$ symbol):
%
\begin{align*}
  \hat{\Beta}_0 & = \frac{1}{n_\mathrm{WT}} \sum_{i = 1}^{n_\mathrm{WT}} \Delta^{(i)} \\
  \hat{\Sigma}  & = \begin{bmatrix}
    \max\{\hat{\sigma}_1^2, \tilde{\sigma}_1^2\} + \hat{\tau}_1^2 &        & 0                                                                         \\
                                                                  & \ddots                                                                             \\
    0                                                             &        & \max\{\hat{\sigma}_{|S|}^2, \tilde{\sigma}_{|S|}^2\} + \hat{\tau}_{|S|}^2 \\
  \end{bmatrix}
\end{align*}
%
where $\Delta^{(i)}$ is the read counts for \gls{wt} sample $i$; $\hat{\sigma}_s^2$ is the raw estimate of the biological variance for transcript $s$; $\tilde{\sigma}_i^2$ is the shrunken estimate of the biological variance for transcript $s$ made through aggregating data across transcripts; and $\hat{\tau}_s^2$ is the estimate of the inferential variance for transcript $s$ obtained from bootstrapping (see \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017} for details).
However, if we treat the estimates $\hat{\Beta}_0$ and $\hat{\Sigma}$ derived from the \gls{wt} samples as known inputs for the mutated model, then \Cref{eqn:single_mut_model} for a single mutated sample meets the criteria for the \gls{js} estimators.
This is not necessarily an accurate assumption, since these estimates may be biased, inaccurate at small samples sizes, or affected by confounding factors, like batch effects.
But with sufficiently large $n_\mathrm{WT}$, this may not be a practical concern.

A \gls{js} estimator for the unknown fold change, $\Beta_1$, can be constructed according to \Cref{thm:js}:
%
\begin{equation}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{(\Delta^{(\mathrm{Mut})} - \hat{\Beta}_0)^\tran \hat{\Sigma}^{-1} (\Delta^{(\mathrm{Mut})} - \hat{\Beta}_0)} \right)(\Delta^{(\mathrm{Mut})} - \hat{\Beta}_0)
  \label{eqn:js_defn}
\end{equation}
%
where $\hat{\Beta}_0$ is the estimate obtained from the non-mutated samples for all transcripts $s \in S$, and $\Delta^{(\mathrm{Mut})}$ is the read counts for all transcripts in the single mutated sample.
By \Cref{eqn:delta_mut}, the following relationships can be derived to satisfy \Cref{thm:js}:
%
\begin{align*}
  \trace{\hat{\Sigma}} & = \sum_{s \in S} \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2                                                                                                                                      \\
  \lambda              & = \max_{s \in S} \left\{ \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right\}                                                                                                                     \\
  0                    & \le c \le 2 \left( \frac{\sum_{s \in S} \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2}{\max_{s \in S} \left\{ \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right\}} - 2 \right) \\
\end{align*}

\subsection{Comparison between the \glsentryshort{ols} and \glsentrylong{js} estimators}

For a simple experimental design where the mutation status is the only coefficient the \gls{ols} estimator is given by:
%
\begin{equation*}
  \begin{bmatrix}
    \hat{\beta}_{s,0}^{(OLS)} \\
    \hat{\beta}_{s,1}^{(OLS)}
  \end{bmatrix}
  = \hat{\beta}_s^{(OLS)}
  = (X^\tran X)^{-1}X^\tran d_s
  = \begin{bmatrix}
    \bar{d}_s^{(wt)} \\
    d_s^{(mut)} - \bar{d}_s^{(wt)}
  \end{bmatrix}
\end{equation*}
%
where
%
\begin{equation*}
  X = \begin{bmatrix}
    1      & 1      \\
    1      & 0      \\
    \vdots & \vdots \\
    1      & 0
  \end{bmatrix}
  \in \mathbb{R}^{(N + 1) \times 2}
\end{equation*}
%
is the design matrix.
Looking closely at the \gls{ols} estimator for the mutation coefficient, $\beta_{s,1}$, it is given by:
%
\begin{equation}
  \hat{\beta}_{s,1}^{(OLS)} = d_s^{(mut)} - \hat{\beta}_{s,0}^{(OLS)} = \delta_s - \hat{\beta}_{0,s}
\end{equation}
%
which is used directly in the definition of the \gls{js} estimator in \Cref{eqn:js_defn}.
The \gls{js} estimator for $\Beta_1$ can then be expressed simply as:
%
\begin{equation}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_1^{(OLS)} \right)^\tran \Sigma^{-1} \hat{\Beta}_1^{(OLS)}} \right) \hat{\Beta}_1^{(OLS)}
  \label{eqn:js_defn_ols}
\end{equation}
%
From this definition, one can see that the \gls{js} estimate is colinear with the \gls{ols} estimate but uniformly shrunk towards 0.
For a more general experimental design, the above can be extended.

\begin{theorem}
  Given an experimental design matrix $X \in \mathbb{R}^{n \times p}$ where $n > p$, $\text{rank}(X) = p$ and $\text{rank}(X^*) = p - 1$ where $X^* \in \mathbb{R}^{(n - 1) \times p}$ is the same design matrix but with one sample removed, a \gls{js} estimator for the linear coefficient uniquely specified by the one sample is given by

  \begin{equation*}
    \hat{\Beta}_i^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_i^{(OLS)} \right)^\tran \Sigma^{-1} \hat{\Beta}_i^{(OLS)}} \right) \hat{\Beta}_i^{(OLS)}
  \end{equation*}
\end{theorem}

It can be shown that the \gls{js} estimator is biased towards 0 with a smaller variance than the \gls{ols} estimator (see \Cref{sec:JS_moments}).
In theory, this may increase the error of some transcripts, but will decrease the \gls{mse} for a set of transcripts in aggregate (see \Cref{sec:JS_moments}).

From this definition, we can see two parameters of this model that will affect the amount of biasing: the scaling coefficient, $c$, and the total number of transcripts being considered, $|S|$.
Firstly, the scaling coefficient can be manually specified, and the largest biasing occurs when $c$ is its maximum value, $2 \left( \frac{\trace{\Sigma}}{\lambda} - 2 \right)$.
Secondly, the transcripts under consideration can also be manually specified, which will affect the value of the denominator $\left( \hat{\Beta}_1^{(OLS)} \right)^\tran \Sigma^{-1} \hat{\Beta}_1^{(OLS)}$, and thus the amount of biasing.
The more transcripts under consideration, the larger the denominator, and thus the smaller the effect, compared to the \gls{ols} method.
Thus, we have produced a high-bias, low-variance fold change estimator that has a lower \gls{mse} than the \gls{ols} estimator and two tunable parameters.

\subsection{Empirical analysis of the \glsentrylong{js} estimator}

Now that the statistical properties of the \gls{js} estimator have been shown, we use empirical \gls{rnaseq} data to measure the performance of this method in practice.
To demonstrate this, we make use of a highly replicated \gls{rnaseq} experiment involving $\Delta$\emph{Snf2} \gls{ko} and \gls{wt} yeast cells \cite{gierlinskiStatisticalModelsRNAseq2015}.
This dataset contains 48 biological replicates of each condition, an infeasible sample size for most \gls{rnaseq} experiments.
Experiments with small sample sizes can be compared to the full dataset to estimate the number of true and false detections for a given experimental design and a given method.
We randomly select $N$ total samples from the full dataset with an optimal even split between the two groups (i.e. $N / 2$ $\Delta$\emph{Snf2} and $N / 2$ \gls{wt}) or a suboptimal $N - 1$-vs-1 split (i.e. $N - 1$ $\Delta$\emph{Snf2} and 1 \gls{wt} or 1 $\Delta$\emph{Snf2} and $N - 1$ \gls{wt}).
To measure the effect of total sample size, these simulations are repeated for multiple values of $N$.

We find that for all values of $N$, the \gls{js} method produces more \gls{tp} and \gls{fp} calls, as well as fewer \gls{tn} and \gls{fn} calls, than the \gls{ols} method with suboptimal designs, on average (two-way \gls{anova}, $p < 2.2 \times 10^{-16}$; \Cref{fig:JS_fig2}).
For example, for $N = 6$, the \gls{js} method identified 642.87 \gls{tp}, 58 \gls{fp}, 2098.3 \gls{tn}, and 2995.7 \gls{fn} calls on average, compared the to 520.7 \gls{tp}, 41.37 \gls{fp}, 2114.97 \gls{tn}, and 3117.93 \gls{fn} calls for the \gls{ols} method (23.5 \% more \gls{tp}, 40 \% more \gls{fp}, 1.8 \% fewer \gls{tp} and 3.9 \% fewer \gls{fn} calls; \Cref{fig:JS_fig2}).
The strength of this effect appears to decrease as the total number of samples increases.
Notably, for the $N = 4$ case where a suboptimal design would be most common in practice, the \gls{js} method had more \gls{tp} and fewer \gls{fn} than the optimal experimental design.
In all other cases, however, the optimal even split between $\Delta $ \emph{Snf2} and \gls{wt} groups results in the most \gls{tp} and fewest \gls{fn} calls, as expected.
Thus, for differential expression hypothesis testing, the \gls{ols} method can identify more \gls{tp} and fewer \gls{fn} calls than the \gls{ols} method when dealing with suboptimal experimental designs.

\newfigure{chapter4/Figure2.png}{Differential gene expression analysis of the entire yeast transcriptome with differently sized experimental designs}{Simulations ($n = 30$) using randomly selected samples which were then compared to the full dataset of 48 $\Delta$\emph{Snf2} vs 48 \gls{wt} to calculate \gls{tp} (\textbf{a}), \gls{fp} (\textbf{b}), \gls{tn} (\textbf{c}), and \gls{fn} (\textbf{d}).}{fig:JS_fig2}

To investigate where the changes in statistical inferences come from, we can view a representative simulation (\Cref{fig:JS_fig3}).
In the full dataset with 48 biological replicates, \emph{Snf2} is the most under-expressed gene with an estimated 99 \% reduction in expression (\Cref{fig:JS_fig3}a).
Three other example genes, \emph{PHO12}, \emph{TIS11}, and \emph{TYE7}, are also under-expressed in the $\Delta$\emph{Snf2} cells.
Using 4 samples in total, evenly split between the two groups, all four genes remain detected as differentially expressed (\Cref{fig:JS_fig3}b).
Using a suboptimal design with 1 $\Delta$\emph{Snf2} and 3 \gls{wt} samples, \emph{TIS11} and \emph{TYE7} are no longer detected as differentially expressed using the \gls{ols} method (\Cref{fig:JS_fig3}c).
However, the \emph{TIS11} gene is detected as differentially expressed using the \gls{js} method (\Cref{fig:JS_fig3}d).
The fold change estimates in \Cref{fig:JS_fig3}c-d are similar, since the biasing is small effect.
Thus, the differences in statistical inference result from the smaller variance of the \gls{js} estimator, as predicted.

\newfigure{chapter4/Figure3.png}{Differential gene expression analysis of $\Delta$\emph{Snf2} vs \gls{wt} yeast cells using different sample sizes and experimental designs}{\textbf{a.} Volcano plot of differential expression results with \gls{ols} estimates in a highly replicated experiment consisting of 48 biological replicates of each condition. \textbf{b.} The same analysis as (\textbf{a}) using 4 samples in total, 2 $\Delta$\emph{Snf2} and 2 \gls{wt} samples. \textbf{c.} The same analysis as (\textbf{c}) using 1 $\Delta$\emph{Snf2} and 3 \gls{wt}. \textbf{d.} The same analysis as (\textbf{c}) using the \gls{js} method instead of \gls{ols}.}{fig:JS_fig3}

Finally, we investigated the impact of the number of transcripts considered on \gls{mse} reduction.
Using the same yeast \gls{rnaseq} data, we randomly selected $N = 6$ samples and a small number of transcripts from the entire transcriptome, then performed differential expression analysis with the \gls{ols} and \gls{js} methods.
This was repeated 30 times (15 simulations of 5 $\Delta$\emph{Snf2} and 1 \gls{wt} and 15 simulations of 1 $\Delta$\emph{Snf2} and 5 \gls{wt}) with a total of $|S| \in \{ 3, 10, 25, 50, 100, 250 \}$ transcripts.
Nearly all simulations show a smaller \gls{mse} when using the \gls{js} method compared to the \gls{ols} method (dots below the dotted lines in \Cref{fig:JS_fig4}a).
Moreover, the \gls{mse} is reduced by 7.73 - 22.68 \% using the \gls{js} method, on average, regardless of the number of transcripts considered.
The largest percentage of \gls{mse} reduction is observed when 3 transcripts are chosen, with an \textapprox 86 \% error reduction (\Cref{fig:JS_fig4}b).
However, the effect is also more variable when fewer transcripts are considered, as some simulations with 3 transcripts resulted in an \textapprox 150 \% increase in error (\Cref{fig:JS_fig4}b).
Taken together, we find that the \gls{js} method significantly reduces the fold change \gls{mse}, with greater reductions found by considering a smaller number of transcripts.

\newfigure{chapter4/Figure4.png}{Differential gene expression analysis focusing on a subset of trancsripts, not the entire transcriptome}{All experiments use 1 $\Delta$\emph{Snf2} vs 5 \gls{wt} samples (or vice versa). \textbf{a.} Comparison of the \gls{mse} of the \gls{js} estimates ($y$-axis) against the \gls{ols} estimates ($x$-axis). The total number of transcripts in each comparison is specified above each facet. \textbf{b.} Percent different in \gls{mse} between the \gls{js} and \gls{ols} estimates. One-sided, two-sample paired Student's $t$-test, $n = 30$, FDR multiple test corrections.}{fig:JS_fig4}
