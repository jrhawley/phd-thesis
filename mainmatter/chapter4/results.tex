\section{Results}

% \newfigure{chapter4/Figure1.png}{Reducing the bias-variance tradeoff by combining information across multiple features}{Schematic of the bias-variance tradeoff for assessing model performance. Dartboard on the left shows low bias of the darts (mean is close to the bullseye) but a large variance. Dartboard on the right shows a high bias of the darts (mean if off-centre), but a small variance. For a p-variate normal distribution from which a single observation is made, the na√Øve estimator has a higher \gls{mse} than the James-Stein estimator. An analogy showing how the James-Stein estimators work in theory. Trying to estimate the mean height, weight, and age for the entire population ($\mu$) from a single person will give an estimate that is likely wrong ($\mu^{(1)}$). Combining information from the three variables together can produce an estimate that is closer to the truth ($\mu^{(2)}$).}{fig:JS_fig1}