\chapter{Supplementary Material for Chapter 4}
\label{chap:JS_appendix}

\section{Notation}
\label{sec:JS_notation}

Below is a summary of the notation and symbols that are used throughout this appendix and \Cref*{chap:JS}.

\newlongtable{l l l}{Operators used throughout \Cref*{chap:JS_appendix} and \Cref*{chap:JS}}{
  Name & Notation & Definition
}{
  tab:JS_appendix_operators
}{
  Expectation & $\expect{ \cdot }$          & Expectation of a random variable. \\
  Covariance  & $\variance{ \cdot, \cdot }$ & Covariance of two random variables. \\
  Variance    & $\variance{ \cdot }$        & Variance of a random variable. \\
  Trace       & $\trace{ \cdot }$           & Trace of a matrix \\
              &                             & (or the matrix representation of a random variable).
}

\newlongtable{l l l l}{Random variables used throughout \Cref*{chap:JS_appendix} and \Cref*{chap:JS}}{
  Name & Notation & Units & Definition
}{
  tab:JS_appendix_variables
}{
  Latent transcript         & $Y$        & None & Abundance of sequencing reads assigned to \\
    abundance               &            &      & each transcript in an organism's transcriptome. \\
  Observed transcript       & $D$        & None & Observed abundance of sequencing reads assigned to \\
    abundance               &            &      & each transcript in an organism's transcriptome. \\
  Biological noise          & $\epsilon$ & None & Differences in expression between samples and \\
                            &            &      & variance from library preparation methods \cite{pimentelDifferentialAnalysisRNAseq2017}. \\
  Inferential noise         & $\zeta$    & None & Differences in expression arising from \\
                            &            &      & computational inference of reads to transcripts \\
                            &            &      & and sequencing stochasticity \cite{pimentelDifferentialAnalysisRNAseq2017}. \\
  \gls{ols} mean estimator  & $\muols$   & None & Estimator of mean fold change using the \gls{ols} method. \\
  \gls{js} mean estimator   & $\mujs$    & None & Estimator of mean fold change using the \gls{js} method. \\
}

\newlongtable{l l l}{Symbols used throughout \Cref*{chap:JS_appendix} and \Cref*{chap:JS}}{
  Name & Notation & Definition
}{
  tab:JS_appendix_symbols
}{
  Estimate            & $\hat{ \cdot }$                & Estimation of some constant or parameter. \\
  Transpose           & $\cdot^\tran$                  & Transpose of a matrix \\
                      &                                & (or the matrix representation of a random variable). \\
  Normal distribution & $\mathcal{N}_p( \mu, \Sigma )$ & A $p$-dimensional normal distribution with mean $\mu$ \\
                      &                                & and covariance matrix $\Sigma$. \\
  Identity matrix     & $I_n$, $I_{r \times c}$        & A square $n \times n$, or rectangular $r \times c$, identity matrix.
}

\newlongtable{l l l}{Parameters and constants used throughout \Cref*{chap:JS_appendix} and \Cref*{chap:JS}}{
  Name & Notation & Definition
}{
  tab:JS_appendix_params
}{
  Design matrix                                 & $X$                & Matrix of covariates as columns and samples as rows. \\
                                                &                    & All $p$ covariates for sample $i$ are denoted by the \\
                                                &                    & $p$-dimensional column vector $x_i$. \\
  Set of transcripts                            & $S$                & All transcripts from an organism's transcriptome that \\
                                                &                    & are considered in a differential expression analysis. \\
  Wild type sample size                         & $n_\mathrm{WT}$    & The number of samples in the "wild type" group. \\
  Mutant sample size                            & $n_\mathrm{Mut}$   & The number of samples in the "mutant" group. \\
                                                &                    & In this application, $n_\mathrm{Mut} = 1$. \\
  Total sample size                             & $n_\mathrm{Tot}$   & Total sample size in a differential expression analysis  \\
                                                &                    & experiment. In this application, $n_\mathrm{Tot} = n_\mathrm{WT} + n_\mathrm{Mut}$. \\
  Variance of inferential noise                 & $\tau^2$           & Inferential noise, $\zeta$, is distributed as $\mathcal{N}_{|S|} \left( 0, \tau^2 \right)$. \\
                                                &                    & By assumption $\tau^2$ is a diagonal matrix where \\
                                                &                    & the $i$-th diagonal element is denoted as $\tau^2_i$. \\
  Variance of biological noise                  & $\sigma^2$         & Biological noise, $\epsilon$, is distributed as $\mathcal{N}_{|S|} \left( 0, \sigma^2 \right)$. \\
                                                &                    & By assumption $\sigma^2$ is a diagonal matrix where \\
                                                &                    & the $i$-th diagonal element is denoted as $\sigma^2_i$. \\
  Shrinkage-based variance of biological noise  & $\tilde{\sigma}^2$ & Biological noise, $\epsilon$, is distributed as $\mathcal{N}_{|S|} \left( 0, \sigma^2 \right)$. \\
                                                &                    & By assumption $\sigma^2$ is a diagonal matrix where \\
                                                &                    & the $i$-th diagonal element is denoted as $\sigma^2_i$. \\
  Estimated count                               & $k_{si}$           & Read abundance of transcript $s$ in sample $i$ as \\
                                                &                    & estimated by pseudo-alignment in Kallisto. \\
  Sample scale factor                           & $f_i$              & Read abundance scale factor for sample $i$ to adjust \\
                                                &                    & for differences in sequencing depth between samples. \\
  Covariate effect                              & $\beta_s$          & Fixed effect of each covariate on the expression of \\
                                                &                    & transcript $s$. In this application, $\beta_{s,0}$ is the \\
                                                &                    & baseline expression and $\beta_{s,1}$ is the fold-change in \\
                                                &                    & mutant samples.
}

\section{Differential expression analysis with Sleuth}

The differential expression model employed in the Sleuth (v0.30.0) \cite{pimentelDifferentialAnalysisRNAseq2017,yiGenelevelDifferentialAnalysis2018} can be described as follows.
Consider a set of transcripts, $S$, that are each measured in $n_\mathrm{Tot}$ samples with an experimental design matrix, $X \in \mathbb{R}^{n_\mathrm{Tot} \times p}$, where $p$ is the number of covariates considered.
Let $Y_{si}$ be the natural log of the abundance of transcript $s$ in sample $i$.
Given the design matrix:
%
\begin{equation}
  X = \begin{bmatrix}
    x_1^\tran \\
    x_2^\tran \\
    ... \\
    x_n^\tran
  \end{bmatrix}
\end{equation}
%
the abundance of transcripts can be modelled as a \gls{glm}:
%
\begin{equation}
  Y_{si} = x_i^\tran \beta_s + \epsilon_{si}
\end{equation}
%
where $\epsilon_{si} \sim \mathcal{N}(0, \sigma_s^2)$ is the biological noise of transcript $s$ in sample $i$ and $\beta_s \in \mathbb{R}^p$ is the fixed effect of the covariates on the expression of transcript $s$.

Due to inferential noise from sequencing, each $Y_{si}$ are not observed directly, but indirectly through the observed perturbations, $D_{si}$.
This can be modelled as:
%
\begin{equation}
  D_{si} | Y_{si} = Y_{si} + \zeta_{si}
\end{equation}
%
where $\zeta_{si} \sim \mathcal{N}(0, \tau_s^2)$ is the inferential noise of transcript $s$ in sample $i$.
Both biological and inferential noise for each transcript are \gls{iid} and independent of each other.
Namely:
%
\begin{align}
  \variance{\epsilon_{si}, \epsilon_{rj}} & =
  \begin{cases}
    \sigma_s^2 & i = j, s = r \\
    0
  \end{cases}                     \\
  \variance{\zeta_{si}, \zeta_{rj}}       & =
  \begin{cases}
    \tau_s^2 & i = j, s = r \\
    0
  \end{cases}                     \\
  \variance{\epsilon_{si}, \zeta_{rj}}    & = 0
\end{align}
%
where $s, r \in S$, and $i, j \in \{1, ..., n_\mathrm{Tot} \}$.
The abundances for transcript $s$ in all $n_\mathrm{Tot}$ samples can then modelled as a multivariate normal distribution:
%
\begin{equation}
  D_{s} \sim \mathcal{N}_{n_\mathrm{Tot}}(X \beta_s, (\sigma_s^2 + \tau_s^2)I_{n_\mathrm{Tot}})
\end{equation}
%
The goal of the differential analysis is to estimate the $p$ coefficients in $\beta_s \forall s \in S$ ($|S| \times p$ in total) and to determine which coefficients differ significantly from 0.
This is often achieved through a Wald test or likelihood ratio test after estimating the inferential variance through bootstrapping and the biological variance through dispersion estimation and shrinkage.
Using these estimation methods on the $n_\mathrm{WT}$ \gls{wt} samples provides the following estimates:
% TODO: FIX THIS AND FIND OUT WHERE IT NEEDS TO GO
\begin{align*}
  \hat{\Beta}_0 & = \frac{1}{n_\mathrm{WT}} \sum_{i = 1}^{n_\mathrm{WT}} \Delta^{(i)} \\
  \hat{\Sigma}  & = \begin{bmatrix}
    \max\{\hat{\sigma}_1^2, \tilde{\sigma}_1^2\} + \hat{\tau}_1^2 &        & 0                                                                         \\
                                                                  & \ddots                                                                             \\
    0                                                             &        & \max\{\hat{\sigma}_{|S|}^2, \tilde{\sigma}_{|S|}^2\} + \hat{\tau}_{|S|}^2 \\
  \end{bmatrix}
\end{align*}
%
where $\Delta^{(i)}$ is the abundance for \gls{wt} sample $i$; $\hat{\sigma}_s^2$ is the raw estimate of the biological variance for transcript $s$; $\tilde{\sigma}_i^2$ is the shrunken estimate of the biological variance for transcript $s$ made through aggregating data across transcripts; and $\hat{\tau}_s^2$ is the estimate of the inferential variance for transcript $s$ \cite{pimentelDifferentialAnalysisRNAseq2017}.

% TODO: FIX THIS AND FIND OUT WHERE IT NEEDS TO GO
By \Cref{eqn:delta_mut}, the following relationships can be derived to satisfy \Cref{thm:js}:
%
\begin{align}
  \trace{\hat{\Sigma}} & = \sum_{s \in S} \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2                                                                                                                                      \\
  \lambda              & = \max_{s \in S} \left\{ \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right\}                                                                                                                     \\
  0                    & \le c \le 2 \left( \frac{\sum_{s \in S} \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2}{\max_{s \in S} \left\{ \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right\}} - 2 \right)
  \label{eqn:js_scaling_bounds}
\end{align}

The estimator for the differential effect is the \gls{ols} estimate:
%
\begin{equation}
  \hat{\beta}_s = (X^\tran X)^{-1} X^\tran D_s
\end{equation}
%
$D_s$ is defined by a transformation of raw read counts to correct for biases such as transcript length and sequencing depth between different samples:
%
\begin{align}
  D_{si}   & = \ln \left(\frac{k_{si}}{f_i} + 0.5 \right)                                                            \\
  f_i & = \underset{s \in S^*}{\mathrm{median}} \frac{k_{si}}{\sqrt[N]{\underset{j = 1}{\overset{N}{\prod }}k_{s,j}}}
\end{align}
%
where $k_{si}$ is the estimated read count from the Kallisto package \cite{brayNearoptimalProbabilisticRNAseq2016} for transcript $s$ in sample $i$ and $f_i$ is the read count scale factor for sample $i$.
The read count scale factor is calculated from the set of all transcripts that pass initial filtering, $S^*$.
This is typically the entire transcriptome, $S$, but excludes transcripts with read counts below some threshold (see \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017} for details).

\section{Statistical moments of the \glsfmtlong{ols} estimator}

As shown in Supplementary Note 2 of \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017}, the estimator is unbiased:
%
\begin{equation}
  \expect{ \hat{\beta}_s^{(OLS)} } = \beta_s
\end{equation}
%
It can also be shown that, for a covariance matrix $\Sigma$,
%
\begin{equation}
  \variance{ \hat{\beta}_s^{(OLS)} } = (X^\tran X)^{-1} X^\tran \Sigma X (X^\tran X)^{-1}
\end{equation}
%
In the case where the covariance matrix is diagonal (i.e. $\Sigma = (\sigma_s^2 + \tau_s^2)I_{n_\mathrm{Tot}}$), this reduces to:
%
\begin{equation}
  \variance{ \hat{\beta_s}^{(OLS)} } = (\sigma_s^2 + \tau_s^2)(X^\tran X)^{-1}
\end{equation}
%
Now consider the simple experimental design from \Cref{chap:JS}, where the only covariate of interest is the presence of a mutation, $n_\mathrm{Mut}$ samples have this mutation, and $n_\mathrm{WT}$ samples do not.
Then the design matrix looks like so:
%
\begin{equation}
  X = \begin{bmatrix}
    1      & 1      \\
    \vdots & \vdots \\
    1      & 1      \\
    1      & 0      \\
    \vdots & \vdots \\
    1      & 0
  \end{bmatrix}
  \in \mathbb{R}^{n_\mathrm{Tot} \times 2}
\end{equation}
%
where the first column corresponds to the mean expression (i.e. the intercept, in statistical terms) and the second column corresponds to the mutation status.
The variance of the \gls{ols} estimator is then:
%
\begin{equation}
  \variance{ \hat{\beta_s}^{(OLS)} } = \frac{(\sigma_s^2 + \tau_s^2)}{n_\mathrm{Mut} n_\mathrm{WT}}
  \begin{bmatrix}
    n_\mathrm{Mut}   & -n_\mathrm{Mut}                \\
    - n_\mathrm{Mut} & n_\mathrm{Mut} + n_\mathrm{WT} \\
  \end{bmatrix}
\end{equation}
%
Importantly, the estimate for the coefficient measuring the effect that the presence of the mutation has the following variance:
%
\begin{equation}
  \variance{ \beta_{s,\mathrm{Mut}}^{(OLS)} } = \frac{(\sigma_s^2 + \tau_s^2)(n_\mathrm{Mut} + n_\mathrm{WT})}{n_\mathrm{Mut} n_\mathrm{WT}}
\end{equation}
%
When there is only a single mutated sample (i.e. $n_\mathrm{Mut} = 1$), this reduces to:
%
\begin{equation}
  \variance{ \beta_{s,\mathrm{Mut}}^{(OLS)} } = \frac{(\sigma_s^2 + \tau_s^2)(1 + n_\mathrm{WT})}{n_\mathrm{WT}}
\end{equation}

\section{Statistical moments of the \glsfmtlong{js} estimator}
\label{sec:JS_moments}

\subsection{Expected value of the \glsfmtlong{js} estimator}

We can use a Taylor expansion around $\Beta_1$ to approximate the expected value of $\hat{\Beta}_1^{(JS)}$.
Using the definition of $\hat{\Beta}_1^{(JS)}$ from \Cref{eqn:js_defn_ols}, let $u = \hat{\Sigma}^{-1/2}\hat{\Beta}_1^{(OLS)}$.
Then we have the following:
%
\begin{align}
  \Vert u \Vert ^2                & = \left( \hat{\Beta}_1^{(OLS)} \right) ^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)}          \\
  \hat{\Beta}_1^{(JS)}            & = \left( 1 - \frac{c}{\Vert u \Vert ^2} \right) \hat{\Sigma}^{1/2} u                           \\
  \expect{ \hat{\Beta}_1^{(JS)} } & = \expect{ \hat{\Beta}_1^{(OLS)} } - c\hat{\Sigma}^{1/2} \expect{ \frac{u}{\Vert u \Vert ^2} } \\
                                  & = \Beta_1 - c\hat{\Sigma}^{1/2}\expect{ \frac{u}{\Vert u \Vert ^2} }
\end{align}
%
Let $a = \hat{\Sigma}^{-1/2}\Beta_1$.
Then:
\begin{equation}
  \Vert a \Vert ^2 = \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1
\end{equation}
%
Expanding $\frac{u}{\Vert u \Vert ^2}$ around gives:
%
\begin{align}
  \expect{ \hat{\Beta}_1^{(JS)} }
   & = \Beta_1 - c\hat{\Sigma}^{1/2}\expect{
    \frac{a}{\Vert a \Vert ^2}
    + \left( \frac{1}{\Vert a \Vert ^2} - \frac{2}{\Vert a \Vert ^4} aa^\tran \right)(u - a)
    + \mathcal{O}(\Vert u - a \Vert ^2)
  }                                                                                \\
   & = \left(1 - \frac{c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \Beta_1
  + \mathcal{O}(\Vert u - a \Vert ^2)
\end{align}
%
For sufficiently small $|S|$ and sufficiently small coefficient of variation (i.e. that $\Vert u - a \Vert ^2 \ll \Vert \Beta_1 \Vert ^2$), the Taylor approximation is approximately:
%
\begin{equation}
  \expect{ \hat{\Beta}_1^{(JS)} } \approx \left(1 - \frac{c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \Beta_1
\end{equation}
%
Thus, the \gls{js} estimator is an estimate of $\Beta_1$ that is biased towards 0.

\subsection{Variance of the \glsfmtlong{js} estimator}

The \gls{mse} of the \gls{js} estimator is related to its variance.
%
\begin{equation*}
  \expect{ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2 }
  = \sum_{s \in S} \expect{ \left( \hat{\Beta}_{1,s}^{(JS)} - \Beta_{1,s} \right)^2 } \\
  = \sum_{s \in S} \variance{ \hat{\Beta}_{1,s}^{(JS)} }
\end{equation*}
%
By \cite[REF][]{bockMinimaxEstimatorsMean1975}, $\mathbb{E} \left[ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2\right] \le \mathbb{E} \left[ \Vert \hat{\Beta}_1^{(OLS)} - \Beta_1 \Vert ^2\right]$.
This does not imply that $\variance{ \hat{\Beta}_{1,s}^{(JS)} } \le \variance{ \hat{\Beta}_{1,s}^{(OLS)} } \forall s \in S$, however.
Some transcripts may have larger variances than the \gls{ols} estimator.
This is still desirable if the goal is to find if there is an effect on any transcripts in the set $S$, instead of a particular one within the set.

To compute the variance of $\hat{\Beta}_1^{(JS)}$, we take a similar approach as above with Taylor expansions.
Again, let $u = \hat{\Sigma}^{-1/2} \hat{\Beta}_1^{(OLS)}$.
Then:
%
\begin{align}
  \hat{\Beta}_1^{(JS)} \left( \hat{\Beta}_1^{(JS)} \right)^\tran & = \hat{\Sigma}^{1/2} \left( 1 - \frac{c}{\Vert u \Vert ^2} \right)^2 u u^\tran \hat{\Sigma}^{1/2}                                                                \\
                                                                 & = \hat{\Sigma}^{1/2} \left[ u u^\tran - \frac{2c}{\Vert u \Vert ^2} u u^\tran + \left( \frac{c}{\Vert u \Vert ^2} \right)^2 u u^\tran \right] \hat{\Sigma}^{1/2}
\end{align}
%
The variance is then given by:
%
\begin{align}
  \variance{ \hat{\Beta}_1^{(JS)} } & = \expect{ \hat{\Beta}_1^{(JS)} \left( \hat{\Beta}_1^{(JS)} \right)^\tran } - \expect{ \hat{\Beta}_1^{(JS)} } \expect{ \hat{\Beta}_1^{(JS)} }^\tran                \\
                                    & \approx \hat{\Sigma}^{1/2} \expect{ u u^\tran - \frac{2c}{\Vert u \Vert ^2} u u^\tran + \left( \frac{c}{\Vert u \Vert ^2} \right)^2 u u^\tran } \hat{\Sigma}^{1/2}
  - \left(1 - \frac{c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right)^2 \Beta_1 \Beta_1^\tran
\end{align}
%
Expanding the expectation about $a = \hat{\Sigma}^{-1/2} \Beta_1$ yields:
%
\begin{equation}
  \variance{ \hat{\Beta}_1^{(JS)} }
  \approx \left(1 - \frac{2c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \hat{\Sigma}
  - \frac{2c}{\left( \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^\tran
\end{equation}
%
Since the diagonal elements of $\frac{2c}{\left( \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^\tran$ are all $\ge 0$ and $0 \le \left(1 - \frac{2c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \le 1 \forall c > 0$, the variance than of the \gls{js} estimators are smaller than the \gls{ols} estimators.
The resulting Wald test statistics for the fold change coefficient of transcript $s$ in the \gls{ols} and \gls{js} cases can be summarized as follows:
%
\begin{align}
  W_s^{(OLS)} & = \frac{ \left( \hat{\Beta}_{1, s}^{(OLS)} \right)^2 }{ \hat{\Sigma}_{s,s} } \\
  W_s^{(JS)}  & = \frac{
  \left( 1 - \frac{ c }{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)} } \right)^2 \left( \hat{\Beta}_{1, s}^{(OLS)} \right)^2
  }{
  \left(1 - \frac{2c}{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)}} \right) \hat{\Sigma}_{s,s}
  - \frac{2c}{\left( \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)} \right)^2} \left( \hat{\Beta}_{1, s}^{(OLS)} \right)^2
  }
\end{align}
%
The coefficient for $\hat{\Beta}_{1,s}^{(OLS)}$ in the numerator
\begin{equation}
  \left( 1 - \frac{ c }{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)} } \right)^2
\end{equation}
%
is larger than the coefficient of $\hat{\Sigma}$ in the denominator
\begin{equation}
  \left(1 - \frac{2c}{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)}} \right)
\end{equation}
%
since $1 - 2d + d^2 > 1 - 2d \forall d \in \mathbb{R}$.
The denominator is also made smaller by the second term involving $\left( \hat{\Beta}_{1,s}^{(OLS)} \right)^2$.
These two terms imply that the Wald test statistics will tend to be larger for the \gls{js} estimator than for the \gls{ols} estimator.
Thus the \gls{js} method will tend produce more positive calls, in general, than the \gls{ols} method.
This can increase the true positive rate of the \gls{js} method, while also potentially increasing the false positive rate.

Notably, the variance of the \gls{js} estimator is a function of both the mean and variance of the transcripts under consideration.
This is in contrast to the \gls{ols} estimator, which is solely a function of the variance.
Additionally, the off-diagonal elements of the matrix $\Beta_1 \Beta_1^\tran$ imply that the \gls{js} fold change estimates are not independent of each other.
This, again, contrasts with the \gls{ols} estimator, where the diagonal covariance matrix, $\hat{\Sigma}$, implies that the fold change estimates are themselves independent of each other.
The effect of this dependence on statistical inference is a function of the variance and true fold change, as can be seen from the $\frac{2c}{\left( \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^\tran$ term.
While often ignored in practice, this statistical dependence can affect the results of statistical inference.
