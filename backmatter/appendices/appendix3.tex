\chapter{Supplementary Material for Chapter 4}

\section{Differential expression analysis with Sleuth}

The differential expression model employed in the Sleuth (v0.30.0) \cite{pimentelDifferentialAnalysisRNAseq2017,yiGenelevelDifferentialAnalysis2018} can be described as follows.
Consider a set of transcripts, $S$, measured in $N$ samples with an experimental design matrix, $X \in \mathbb{R}^{N \times p}$, where $p$ is the number of covariates considered.
Let $Y_{si}$ be the natural log of the abundance of transcript $s$ in sample $i$.
Given the design matrix

\begin{equation*}
X = [x_1^T; x_2^T; ... x_n^T], x_i \in \mathbb{R}^p
\end{equation*}

the abundance of transcripts can be modelled as a \gls{glm}

\begin{equation}
Y_{si} = x_i^T \beta_s + \epsilon_{si}
\end{equation}

where $\epsilon_{si} \sim \mathcal{N}(0, \sigma_s^2)$ is the biological noise of transcript $s$ in sample $i$ and $B_s \in \mathbb{R}^p$ is the fixed effect of the covariates on the expression of transcript $s$.

Due to inferential noise from sequencing, each $Y_{si}$ are not observed directly, but indirectly through the observed perturbations, $D_{si}$.
This can be modelled as

\begin{equation}
D_{si} | Y_{si} = Y_{si} + \zeta_{si}
\end{equation}

where $\zeta_{si} \sim \mathcal{N}(0, \tau_s^2)$ is the inferential noise of transcript $s$ in sample $i$.
Both biological and inferential noise for each transcript are \gls{iid} and independent of each other.
Namely:

\begin{align*}
\mathbb{C}ov [\epsilon_{si}, \epsilon_{rj}] &= \sigma_s^2\delta_{i,j}\delta_{s,r} \\
\mathbb{C}ov [\zeta_{si}, \zeta_{rj}] &= \tau_s^2\delta_{i,j}\delta_{s,r} \\
\mathbb{C}ov [\epsilon_{si}, \zeta_{rj}] &= 0 \\
  & \forall s,r \forall i,j
\end{align*}

The abundances for transcript $s$ in all $N$ samples can then modelled as a multivariate normal distribution

\begin{equation}
  D_{s} | Y_{s} \sim \mathcal{N}_N(X \beta_s, (\sigma_s^2 + \tau_s^2)I_{N})
\end{equation}

where $I_N \in \mathbb{R}^{N \times N}$ is the identity matrix.

The goal of the differential analysis is to estimate the $|S| \times p$ coefficients in $B_s \forall s \in S$, and to determine which coefficients differ significantly from 0.
This is achieved through a Wald test or likelihood ratio test after estimating the inferential variance, $\tau_s^2$, through bootstrapping and the biological variance, $\sigma_s^2$, through dispersion estimation and shrinkage.

The estimator for the differential effect is the \gls{ols} estimate:

\begin{equation*}
  \hat{\beta}_s = (X^TX)^{-1} X^T d_s
\end{equation*}

where $d_s$ is the observed abundances given by

\begin{align*}
  d_{si} &= \ln \left(\frac{k_{si}}{\hat{f_i}} + 0.5 \right) \\
  \hat{f_i} &= \underset{s \in S^*}{\mathrm{median}} \frac{k_{si}}{\sqrt[N]{\underset{j = 1}{\overset{N}{\prod }}k_{sj}}} \\
\end{align*}

where $k_{si}$ is the estimated read count from the Kallisto package (v0.46.1) \cite{brayNearoptimalProbabilisticRNAseq2016} for transcript $s$ in sample $i$ and $\hat{f_i}$ is the scaling factor for sample $i$, calculated from the set of all transcripts that pass initial filtering, $S^*$.

\section{Statistical moments of the \glsentrylong{ols} estimator}

As shown in Supplementary Note 2 of \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017}, the estimator is unbiased, Namely

\begin{equation}
  \mathbb{E} \left[ \hat{\beta}_s^{(OLS)} \right] = B_s
\end{equation}

It can also be shown that, for a covariance matrix $\Sigma$,

\begin{equation*}
  \mathbb{V} \left[ \hat{\beta}_s^{(OLS)} \right] = (X^TX)^{-1} X^T \Sigma X (X^TX)^{-1}
\end{equation*}

In the case where $\Sigma = (\sigma_s^2 + \tau_s^2)I_N$, this reduces to

\begin{equation*}
  \mathbb{V} \left[ \hat{\beta_s}^{(OLS)} \right] = (\sigma_s^2 + \tau_s^2)(X^TX)^{-1}
\end{equation*}

Consider a simple experimental design where the only covariate of interest is the presence of a mutation.
Then the design matrix, with the first column being the intercept and the second being the mutation status, looks like so:

\begin{equation*}
  X = \begin{bmatrix}
  1 & 1 \\
  1 & 0 \\
  \vdots & \vdots \\
  1 & 0
  \end{bmatrix}
\in \mathbb{R}^{(N + 1) \times 2}
\end{equation*}

The variance of the \gls{ols} estimator is then

\begin{equation*}
  \mathbb{V} \left[ \hat{\beta_s}^{(OLS)} \right] = \frac{(\sigma_s^2 + \tau_s^2)}{n_{mut} n_{nonmut}}
  \begin{bmatrix}
    n_{mut} & -n_{mut} \\
    - n_{mut} & n_{mut} + n_{nonmut} \\
  \end{bmatrix}
\end{equation*}

Importantly, the estimate for the coefficient measuring the effect that the presence of the mutation has variance

\begin{equation*}
  \mathbb{V} \left[\beta_{s, mut}^{(OLS)} \right] = \frac{(\sigma_s^2 + \tau_s^2)(n_{mut} + n_{nonmut})}{n_{mut} n_{nonmut}}
\end{equation*}

When there is only 1 mutated sample, as per the motivation of this work, this reduces to

\begin{equation}
  \mathbb{V} \left[\beta_{s, mut}^{(OLS)} \right] = \frac{(\sigma_s^2 + \tau_s^2)(1 + n_{nonmut})}{n_{nonmut}}
\end{equation}



\section{Statistical moments of the \glsentrylong{js} estimator}
\label{sec:JS_moments}

\subsection{Expected value of the \glsentrylong{js} estimator}

Due to the non-linear nature of the \gls{js} estimator, a Taylor expansion around $\Beta_1$ can be used to approximate the expectation.
Consider:

\begin{equation*}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_1^{(OLS)} \right)^T \Sigma^{-1} \hat{\Beta}_1^{(OLS)}} \right) \hat{\Beta}_1^{(OLS)}
\end{equation*}

where

\begin{align*}
  \hat{\Beta}_1^{(OLS)} &\sim N_{|S|}(\Beta_1, \Sigma) \\
  \Sigma_{s,s} &= \left( \frac{n_{nonmut} + 1}{n_{nonmut}} \right) (\sigma_t^2 + \tau_t^2) \\
  \Sigma_{s,t} &= 0 \forall t \ne s
\end{align*}

Let $u = \Sigma^{-1/2}\hat{\Beta}_1^{(OLS)}$.
Then

\begin{align*}
  \mathbb{E}\left[ \hat{\Beta}_1^{(JS)} \right] &= \mathbb{E} \left[ \hat{\Beta}_1^{(OLS)} \right] - c\Sigma^{1/2}\mathbb{E} \left[ \frac{u}{\Vert u \Vert ^2} \right] \\
  &= \Beta_1 - c\Sigma^{1/2}\mathbb{E} \left[ \frac{u}{\Vert u \Vert ^2} \right] \Sigma^{1/2}\\
\end{align*}

Expanding $\frac{u}{\Vert u \Vert ^2}$ around $a = \Sigma^{-1/2}\Beta_1$ gives:

\begin{align*}
  \mathbb{E}\left[ \hat{\Beta}_1^{(JS)} \right]
  &= \Beta_1 - c\Sigma^{1/2}\mathbb{E} \left[ \frac{a}{\Vert a \Vert ^2} + \left( \frac{1}{\Vert a \Vert ^2}
    - \frac{2}{\Vert a \Vert ^4} aa^T\right)(u - a) + \mathcal{O}(\Vert u - a \Vert ^2)\right] \\
  & = \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Beta_1
    + \mathcal{O}(\Vert u - a \Vert ^2)
\end{align*}

As long as the number of transcripts being considered, $|S|$, is not large, and that the true coefficient of variation is not large (i.e. that $\Vert u - a \Vert ^2 \ll \Vert \Beta_1 \Vert ^2$), the Taylor approximation is close to

\begin{equation}
  \mathbb{E}\left[ \hat{\Beta}_1^{(JS)} \right] \approx \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Beta_1
\end{equation}

Thus the \gls{js} estimator is an estimate of $\Beta_1$ that is biased towards 0.

\subsection{Variance of the \glsentrylong{js} estimator}

The \gls{mse} of the \gls{js} estimator is related to its variance.

\begin{equation*}
  \mathbb{E} \left[ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2\right] 
  = \sum_{s \in S} \mathbb{E}\left[ \left( \hat{\Beta}_{1,s}^{(JS)} - \Beta_{1,s} \right)^2 \right] \\
  = \sum_{s \in S} \mathbb{V}\left[ \hat{\Beta}_{1,s}^{(JS)} \right]
\end{equation*}

By \cite[REF][]{bockMinimaxEstimatorsMean1975}, $\mathbb{E} \left[ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2\right] \le \mathbb{E} \left[ \Vert \hat{\Beta}_1^{(OLS)} - \Beta_1 \Vert ^2\right]$.
However, this does not imply that $\mathbb{V} \left[ \hat{\Beta}_{1,s}^{(JS)} \right] \le \mathbb{V} \left[ \hat{\Beta}_{1,s}^{(OLS)} \right] \forall s \in S$.
Some transcripts may have larger variances than the \gls{ols} estimator, but all transcripts in aggregate will have a smaller \gls{mse}.
This is still desirable if the goal is to find if there is an effect on any transcripts in the set $S$, instead of a particular one within the set.

To calculate the variance for each individual transcript, a similar approach with Taylor expansions can be used, as above.

\begin{align*}
  &\mathbb{V} \left[ \hat{\Beta}_1^{(JS)} \right] \\
  &\approx \mathbb{E} \left[ \hat{\Beta}_1^{(JS)}
    \left( \hat{\Beta}_1^{(JS)} \right)^T \right]
    - \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right)^2 \Beta_1 \Beta_1^T \\
  &= \Sigma^{1/2} \mathbb{E} \left[ u u^T - \frac{2c}{u^Tu}uu^T + \left( \frac{c}{u^Tu} \right)^2 uu^T \right]\Sigma^{1/2}
    - \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right)^2 \Beta_1 \Beta_1^T \\
\end{align*}

where, again, $u = \Sigma^{-1/2} \hat{\Beta}_1^{(OLS)}$.
Expanding about $a = \Sigma^{-1/2} \Beta_1$ yields:

\begin{equation*}
  \mathbb{V} \left [ \hat{\Beta}_1^{(JS)} \right]
  = \left(1 - \frac{2c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Sigma
    - \frac{2c}{\left( \Beta_1^T \Sigma^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^T
    + \mathcal{O}(\Vert u - a \Vert ^4)
\end{equation*}

Under similar conditions of the number of transcripts under consideration, $|S|$, and $\Vert u - a \Vert ^2$, we then have that

\begin{equation}
  \mathbb{V} \left [ \hat{\Beta}_1^{(JS)} \right]
  \approx \left(1 - \frac{2c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Sigma
  - \frac{2c}{\left( \Beta_1^T \Sigma^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^T
\end{equation}

Since the diagonal elements of $\frac{2c}{\left( \Beta_1^T \Sigma^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^T$ are all $\ge 0$ and $0 \le \left(1 - \frac{2c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \le 1 \forall c > 0$, the variance than of the \gls{js} estimators are smaller than the \gls{ols} estimators.
This smaller variance allows for more powerful statistical inferences (i.e. when using a Wald test).

Notably, the variance of the \gls{js} estimator is a function of both the mean and variance of the transcripts under consideration.
This is in contrast to the \gls{ols} estimator, which is solely a function of the variance.
Additionally, the off-diagonal elements of the matrix $\Beta_1 \Beta_1^T$ imply that the \gls{js} fold change estimates are not independent of each other.
This, again, contrasts with the \gls{ols} estimator, where the diagonal covariance matrix, $\Sigma$, implies that the fold change estimates are themselves independent of each other.
The effect of this dependence on statistical inference is a function of the variance and true fold change, as can be seen from the $\frac{2c}{\left( \Beta_1^T \Sigma^{-1} \Beta_1 \right)^2}$ coefficient.
While rarely true in practice, this statistical dependence can affect the results of statistical inference, in theory.
For most purposes, is not expected to have a large effect on the results of statistical inference.
