\chapter{Supplementary Material for Chapter 4}

\section{Differential expression analysis with Sleuth}

The differential expression model employed in the Sleuth (v0.30.0) \cite{pimentelDifferentialAnalysisRNAseq2017,yiGenelevelDifferentialAnalysis2018} can be described as follows.
Consider a set of transcripts, $S$, measured in $N$ samples with an experimental design matrix, $X \in \mathbb{R}^{N \times p}$, where $p$ is the number of covariates considered.
Let $Y_{si}$ be the natural log of the abundance of transcript $s$ in sample $i$.
Given the design matrix:
%
\begin{equation}
  X = [x_1^\tran; x_2^\tran; ... x_n^\tran], x_i \in \mathbb{R}^p
\end{equation}
%
the abundance of transcripts can be modelled as a \gls{glm}:
%
\begin{equation}
  Y_{si} = x_i^\tran \beta_s + \epsilon_{si}
\end{equation}
%
where $\epsilon_{si} \sim \mathcal{N}(0, \sigma_s^2)$ is the biological noise of transcript $s$ in sample $i$ and $B_s \in \mathbb{R}^p$ is the fixed effect of the covariates on the expression of transcript $s$.

Due to inferential noise from sequencing, each $Y_{si}$ are not observed directly, but indirectly through the observed perturbations, $D_{si}$.
This can be modelled as:
%
\begin{equation}
  D_{si} | Y_{si} = Y_{si} + \zeta_{si}
\end{equation}
%
where $\zeta_{si} \sim \mathcal{N}(0, \tau_s^2)$ is the inferential noise of transcript $s$ in sample $i$.
Both biological and inferential noise for each transcript are \gls{iid} and independent of each other.
Namely:
%
\begin{align}
  \variance{\epsilon_{si}, \epsilon_{rj}} & =
  \begin{cases}
    \sigma_s^2 & i = j, s = r \\
    0
  \end{cases}                     \\
  \variance{\zeta_{si}, \zeta_{rj}}       & =
  \begin{cases}
    \tau_s^2 & i = j, s = r \\
    0
  \end{cases}                     \\
  \variance{\epsilon_{si}, \zeta_{rj}}    & = 0
\end{align}
%
where $\variance{\cdot, \cdot}$ is the covariance operator, $s, r \in S$, and $i, j \in \{1, ..., N \}$.
The abundances for transcript $s$ in all $N$ samples can then modelled as a multivariate normal distribution:
%
\begin{align}
  Y_{s} & \sim \mathcal{N}_N(X \beta_s, \sigma_s^2 I_{N})             \\
  D_{s} & \sim \mathcal{N}_N(X \beta_s, (\sigma_s^2 + \tau_s^2)I_{N})
\end{align}
%
where $I_N \in \mathbb{R}^{N \times N}$ is the identity matrix.

The goal of the differential analysis is to estimate the $|S| \times p$ coefficients in $B_s \forall s \in S$, and to determine which coefficients differ significantly from 0.
This is often achieved through a Wald test or likelihood ratio test after estimating the inferential variance, $\tau_s^2$, through bootstrapping and estimating the biological variance, $\sigma_s^2$, through dispersion estimation and shrinkage.
The estimator for the differential effect is the \gls{ols} estimate:
%
\begin{equation}
  \hat{\beta}_s = (X^\tran X)^{-1} X^\tran D_s
\end{equation}
%
$D_s$ is defined by a transformation of raw read counts to correct for biases such as transcript length and sequencing depth between different samples:
%
\begin{align}
  D_{s,i}   & = \ln \left(\frac{k_{s,i}}{\hat{f_i}} + 0.5 \right)                                                            \\
  \hat{f_i} & = \underset{s \in S^*}{\mathrm{median}} \frac{k_{s,i}}{\sqrt[N]{\underset{j = 1}{\overset{N}{\prod }}k_{s,j}}}
\end{align}
%
where $k_{s,i}$ is the estimated read count from the Kallisto package \cite{brayNearoptimalProbabilisticRNAseq2016} for transcript $s$ in sample $i$ and $\hat{f_i}$ is the read count scale factor for sample $i$.
The read count scale factor is calculated from the set of all transcripts that pass initial filtering, $S^*$, which is typically the entire transcriptome, $S$, less a few transcripts with read counts below some threshold (see \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017} for details).

\section{Statistical moments of the \glsentrylong{ols} estimator}

As shown in Supplementary Note 2 of \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017}, the estimator is unbiased:
%
\begin{equation}
  \expect{ \hat{\beta}_s^{(OLS)} } = \beta_s
\end{equation}
%
It can also be shown that, for a covariance matrix $\Sigma$,
%
\begin{equation}
  \variance{ \hat{\beta}_s^{(OLS)} } = (X^\tran X)^{-1} X^\tran \Sigma X (X^\tran X)^{-1}
\end{equation}
%
In the case where the covariance matrix is diagonal (i.e. $\Sigma = (\sigma_s^2 + \tau_s^2)I_N$), this reduces to:
%
\begin{equation}
  \variance{ \hat{\beta_s}^{(OLS)} } = (\sigma_s^2 + \tau_s^2)(X^\tran X)^{-1}
\end{equation}
%
Now consider the simple experimental design from \Cref{chap:JS}, where the only covariate of interest is the presence of a mutation and $n_\mathrm{Mut}$ samples have this mutation and $n_\mathrm{WT}$ samples do not
Then, the design matrix looks like so:
%
\begin{equation}
  X = \begin{bmatrix}
    1      & 1      \\
    \vdots & \vdots \\
    1      & 1      \\
    1      & 0      \\
    \vdots & \vdots \\
    1      & 0
  \end{bmatrix}
  \in \mathbb{R}^{N \times 2}
\end{equation}
%
where the first column corresponds to the mean expression (i.e. the intercept, in statistical terms) and the second column corresponds to the mutation status.
The variance of the \gls{ols} estimator is then:
%
\begin{equation}
  \variance{ \hat{\beta_s}^{(OLS)} } = \frac{(\sigma_s^2 + \tau_s^2)}{n_\mathrm{Mut} n_\mathrm{WT}}
  \begin{bmatrix}
    n_\mathrm{Mut}   & -n_\mathrm{Mut}                \\
    - n_\mathrm{Mut} & n_\mathrm{Mut} + n_\mathrm{WT} \\
  \end{bmatrix}
\end{equation}
%
Importantly, the estimate for the coefficient measuring the effect that the presence of the mutation has the following variance:
%
\begin{equation}
  \variance{ \beta_{s,\mathrm{Mut}}^{(OLS)} } = \frac{(\sigma_s^2 + \tau_s^2)(n_\mathrm{Mut} + n_\mathrm{WT})}{n_\mathrm{Mut} n_\mathrm{WT}}
\end{equation}
%
When there is only a single mutated sample (i.e. $n_\mathrm{Mut} = 1$), this reduces to:
%
\begin{equation}
  \variance{ \beta_{s,\mathrm{Mut}}^{(OLS)} } = \frac{(\sigma_s^2 + \tau_s^2)(1 + n_\mathrm{WT})}{n_\mathrm{WT}}
\end{equation}

\section{Statistical moments of the \glsentrylong{js} estimator}
\label{sec:JS_moments}

\subsection{Expected value of the \glsentrylong{js} estimator}

We can use a Taylor expansion around $\Beta_1$ to approximate the expected value of $\hat{\Beta}_1^{(JS)}$.
Using the definition of $\hat{\Beta}_1^{(JS)}$ from \Cref{eqn:js_defn_ols}, let $u = \hat{\Sigma}^{-1/2}\hat{\Beta}_1^{(OLS)}$.
Then we have the following:
%
\begin{align}
  \Vert u \Vert ^2                & = \left( \hat{\Beta}_1^{(OLS)} \right) ^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)}          \\
  \hat{\Beta}_1^{(JS)}            & = \left( 1 - \frac{c}{\Vert u \Vert ^2} \right) \hat{\Sigma}^{1/2} u                           \\
  \expect{ \hat{\Beta}_1^{(JS)} } & = \expect{ \hat{\Beta}_1^{(OLS)} } - c\hat{\Sigma}^{1/2} \expect{ \frac{u}{\Vert u \Vert ^2} } \\
                                  & = \Beta_1 - c\hat{\Sigma}^{1/2}\expect{ \frac{u}{\Vert u \Vert ^2} }
\end{align}
%
Let $a = \hat{\Sigma}^{-1/2}\Beta_1$.
Then:
\begin{equation}
  \Vert a \Vert ^2 = \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1
\end{equation}
%
Expanding $\frac{u}{\Vert u \Vert ^2}$ around gives:
%
\begin{align}
  \expect{ \hat{\Beta}_1^{(JS)} }
   & = \Beta_1 - c\hat{\Sigma}^{1/2}\expect{
    \frac{a}{\Vert a \Vert ^2}
    + \left( \frac{1}{\Vert a \Vert ^2} - \frac{2}{\Vert a \Vert ^4} aa^\tran \right)(u - a)
    + \mathcal{O}(\Vert u - a \Vert ^2)
  }                                                                                \\
   & = \left(1 - \frac{c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \Beta_1
  + \mathcal{O}(\Vert u - a \Vert ^2)
\end{align}
%
For sufficiently small $|S|$ and sufficiently small coefficient of variation (i.e. that $\Vert u - a \Vert ^2 \ll \Vert \Beta_1 \Vert ^2$), the Taylor approximation is approximately:
%
\begin{equation}
  \expect{ \hat{\Beta}_1^{(JS)} } \approx \left(1 - \frac{c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \Beta_1
\end{equation}
%
Thus, the \gls{js} estimator is an estimate of $\Beta_1$ that is biased towards 0.

\subsection{Variance of the \glsentrylong{js} estimator}

The \gls{mse} of the \gls{js} estimator is related to its variance.
%
\begin{equation*}
  \expect{ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2 }
  = \sum_{s \in S} \expect{ \left( \hat{\Beta}_{1,s}^{(JS)} - \Beta_{1,s} \right)^2 } \\
  = \sum_{s \in S} \variance{ \hat{\Beta}_{1,s}^{(JS)} }
\end{equation*}
%
By \cite[REF][]{bockMinimaxEstimatorsMean1975}, $\mathbb{E} \left[ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2\right] \le \mathbb{E} \left[ \Vert \hat{\Beta}_1^{(OLS)} - \Beta_1 \Vert ^2\right]$.
This does not imply that $\variance{ \hat{\Beta}_{1,s}^{(JS)} } \le \variance{ \hat{\Beta}_{1,s}^{(OLS)} } \forall s \in S$, however.
Some transcripts may have larger variances than the \gls{ols} estimator.
This is still desirable if the goal is to find if there is an effect on any transcripts in the set $S$, instead of a particular one within the set.

To compute the variance of $\hat{\Beta}_1^{(JS)}$, we take a similar approach as above with Taylor expansions.
Again, let $u = \hat{\Sigma}^{-1/2} \hat{\Beta}_1^{(OLS)}$.
Then:
%
\begin{align}
  \hat{\Beta}_1^{(JS)} \left( \hat{\Beta}_1^{(JS)} \right)^\tran & = \hat{\Sigma}^{1/2} \left( 1 - \frac{c}{\Vert u \Vert ^2} \right)^2 u u^\tran \hat{\Sigma}^{1/2}                                                                \\
                                                                 & = \hat{\Sigma}^{1/2} \left[ u u^\tran - \frac{2c}{\Vert u \Vert ^2} u u^\tran + \left( \frac{c}{\Vert u \Vert ^2} \right)^2 u u^\tran \right] \hat{\Sigma}^{1/2}
\end{align}
%
The variance is then given by:
%
\begin{align}
  \variance{ \hat{\Beta}_1^{(JS)} } & = \expect{ \hat{\Beta}_1^{(JS)} \left( \hat{\Beta}_1^{(JS)} \right)^\tran } - \expect{ \hat{\Beta}_1^{(JS)} } \expect{ \hat{\Beta}_1^{(JS)} }^\tran                \\
                                    & \approx \hat{\Sigma}^{1/2} \expect{ u u^\tran - \frac{2c}{\Vert u \Vert ^2} u u^\tran + \left( \frac{c}{\Vert u \Vert ^2} \right)^2 u u^\tran } \hat{\Sigma}^{1/2}
  - \left(1 - \frac{c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right)^2 \Beta_1 \Beta_1^\tran
\end{align}
%
Expanding the expectation about $a = \hat{\Sigma}^{-1/2} \Beta_1$ yields:
%
\begin{equation}
  \variance{ \hat{\Beta}_1^{(JS)} }
  \approx \left(1 - \frac{2c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \hat{\Sigma}
  - \frac{2c}{\left( \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^\tran
\end{equation}
%
Since the diagonal elements of $\frac{2c}{\left( \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^\tran$ are all $\ge 0$ and $0 \le \left(1 - \frac{2c}{\Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1} \right) \le 1 \forall c > 0$, the variance than of the \gls{js} estimators are smaller than the \gls{ols} estimators.
The resulting Wald test statistics for the fold change coefficient of transcript $s$ in the \gls{ols} and \gls{js} cases can be summarized as follows:
%
\begin{align}
  W_s^{(OLS)} & = \frac{ \left( \hat{\Beta}_{1, s}^{(OLS)} \right)^2 }{ \hat{\Sigma}_{s,s} } \\
  W_s^{(JS)}  & = \frac{
  \left( 1 - \frac{ c }{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)} } \right)^2 \left( \hat{\Beta}_{1, s}^{(OLS)} \right)^2
  }{
  \left(1 - \frac{2c}{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)}} \right) \hat{\Sigma}_{s,s}
  - \frac{2c}{\left( \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)} \right)^2} \left( \hat{\Beta}_{1, s}^{(OLS)} \right)^2
  }
\end{align}
%
The coefficient for $\hat{\Beta}_{1,s}^{(OLS)}$ in the numerator
\begin{equation}
  \left( 1 - \frac{ c }{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)} } \right)^2
\end{equation}
%
is larger than the coefficient of $\hat{\Sigma}$ in the denominator
\begin{equation}
  \left(1 - \frac{2c}{ \left( \hat{\Beta}_1^{(OLS)} \right)^\tran \hat{\Sigma}^{-1} \hat{\Beta}_1^{(OLS)}} \right)
\end{equation}
%
since $1 - 2d + d^2 > 1 - 2d \forall d \in \mathbb{R}$.
The denominator is also made smaller by the second term involving $\left( \hat{\Beta}_{1,s}^{(OLS)} \right)^2$.
These two terms imply that the Wald test statistics will tend to be larger for the \gls{js} estimator than for the \gls{ols} estimator.
Thus the \gls{js} method will tend produce more positive calls, in general, than the \gls{ols} method.
This can increase the true positive rate of the \gls{js} method, while also potentially increasing the false positive rate.

Notably, the variance of the \gls{js} estimator is a function of both the mean and variance of the transcripts under consideration.
This is in contrast to the \gls{ols} estimator, which is solely a function of the variance.
Additionally, the off-diagonal elements of the matrix $\Beta_1 \Beta_1^\tran$ imply that the \gls{js} fold change estimates are not independent of each other.
This, again, contrasts with the \gls{ols} estimator, where the diagonal covariance matrix, $\hat{\Sigma}$, implies that the fold change estimates are themselves independent of each other.
The effect of this dependence on statistical inference is a function of the variance and true fold change, as can be seen from the $\frac{2c}{\left( \Beta_1^\tran \hat{\Sigma}^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^\tran$ term.
While often ignored in practice, this statistical dependence can affect the results of statistical inference.
