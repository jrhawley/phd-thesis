\chapter{Supplementary Material for Chapter 4}

\section{Differential expression analysis with Sleuth}

The differential expression model employed in the Sleuth (v0.30.0) \cite{pimentelDifferentialAnalysisRNAseq2017,yiGenelevelDifferentialAnalysis2018} can be described as follows.
Consider a set of transcripts, $S$, measured in $N$ samples with an experimental design matrix, $X \in \mathbb{R}^{N \times p}$, where $p$ is the number of covariates considered.
Let $Y_{si}$ be the natural log of the abundance of transcript $s$ in sample $i$.
Given the design matrix

\begin{equation*}
X = [x_1^T; x_2^T; ... x_n^T], x_i \in \mathbb{R}^p
\end{equation*}

the abundance of transcripts can be modelled as a \gls{glm}

\begin{equation}
Y_{si} = x_i^T \beta_s + \epsilon_{si}
\end{equation}

where $\epsilon_{si} \sim \mathcal{N}(0, \sigma_s^2)$ is the biological noise of transcript $s$ in sample $i$ and $B_s \in \mathbb{R}^p$ is the fixed effect of the covariates on the expression of transcript $s$.

Due to inferential noise from sequencing, each $Y_{si}$ are not observed directly, but indirectly through the observed perturbations, $D_{si}$.
This can be modelled as

\begin{equation}
D_{si} | Y_{si} = Y_{si} + \zeta_{si}
\end{equation}

where $\zeta_{si} \sim \mathcal{N}(0, \tau_s^2)$ is the inferential noise of transcript $s$ in sample $i$.
Both biological and inferential noise for each transcript are \gls{iid} and independent of each other.
Namely:

\begin{align*}
\mathbb{C}ov [\epsilon_{si}, \epsilon_{rj}] &= \sigma_s^2\delta_{i,j}\delta_{s,r} \\
\mathbb{C}ov [\zeta_{si}, \zeta_{rj}] &= \tau_s^2\delta_{i,j}\delta_{s,r} \\
\mathbb{C}ov [\epsilon_{si}, \zeta_{rj}] &= 0 \\
  & \forall s,r \forall i,j
\end{align*}

The abundances for transcript $s$ in all $N$ samples can then modelled as a multivariate normal distribution

\begin{equation}
  D_{s} | Y_{s} \sim \mathcal{N}_N(X \beta_s, (\sigma_s^2 + \tau_s^2)I_{N})
\end{equation}

where $I_N \in \mathbb{R}^{N \times N}$ is the identity matrix.

The goal of the differential analysis is to estimate the $|S| \times p$ coefficients in $B_s \forall s \in S$, and to determine which coefficients differ significantly from 0.
This is achieved through a Wald test or likelihood ratio test after estimating the inferential variance, $\tau_s^2$, through bootstrapping and the biological variance, $\sigma_s^2$, through dispersion estimation and shrinkage.

The estimator for the differential effect is the \gls{ols} estimate:

\begin{equation*}
  \hat{\beta}_s = (X^TX)^{-1} X^T d_s
\end{equation*}

where $d_s$ is the observed abundances given by

\begin{align*}
  d_{si} &= \ln \left(\frac{k_{si}}{\hat{f_i}} + 0.5 \right) \\
  \hat{f_i} &= \underset{s \in S^*}{\mathrm{median}} \frac{k_{si}}{\sqrt[N]{\underset{j = 1}{\overset{N}{\prod }}k_{sj}}} \\
\end{align*}


where $k_{si}$ is the estimated read count from the Kallisto package (v0.46.1) \cite{brayNearoptimalProbabilisticRNAseq2016} for transcript $s$ in sample $i$ and $\hat{f_i}$ is the scaling factor for sample $i$, calculated from the set of all transcripts that pass initial filtering, $S^*$.

\section{Bias and variance of the \glsentrylong{ols} estimator}

As shown in Supplementary Note 2 of \cite[REF][]{pimentelDifferentialAnalysisRNAseq2017}, the estimator is unbiased, Namely

\begin{equation}
  \mathbb{E} \left[ \hat{\beta}_s^{(OLS)} \right] = B_s
\end{equation}

It can also be shown that, for a covariance matrix $\Sigma$,

\begin{equation*}
  \mathbb{V} \left[ \hat{\beta}_s^{(OLS)} \right] = (X^TX)^{-1} X^T \Sigma X (X^TX)^{-1}
\end{equation*}

In the case where $\Sigma = (\sigma_s^2 + \tau_s^2)I_N$, this reduces to

\begin{equation*}
  \mathbb{V} \left[ \hat{\beta_s}^{(OLS)} \right] = (\sigma_s^2 + \tau_s^2)(X^TX)^{-1}
\end{equation*}

Consider a simple experimental design where our only covariate of interest is the presence of a mutation.
Then our design matrix, with the first column being the intercept and the second being the mutation status, looks like so:

\begin{equation*}
  X = \begin{bmatrix}
  1 & 1 \\
  1 & 0 \\
  \vdots & \vdots \\
  1 & 0
  \end{bmatrix}
\in \mathbb{R}^{(N + 1) \times 2}
\end{equation*}

The variance of the \gls{ols} estimator is then

\begin{equation*}
  \mathbb{V} \left[ \hat{\beta_s}^{(OLS)} \right] = \frac{(\sigma_s^2 + \tau_s^2)}{n_{mut} n_{nonmut}}
  \begin{bmatrix}
    n_{mut} & -n_{mut} \\
    - n_{mut} & n_{mut} + n_{nonmut} \\
  \end{bmatrix}
\end{equation*}

Importantly, the estimate for the coefficient measuring the effect that the presence of the mutation has variance

\begin{equation*}
  \mathbb{V} \left[\beta_{s, mut}^{(OLS)} \right] = \frac{(\sigma_s^2 + \tau_s^2)(n_{mut} + n_{nonmut})}{n_{mut} n_{nonmut}}
\end{equation*}

When there is only 1 mutated sample, as per the motivation of this work, this reduces to

\begin{equation}
  \mathbb{V} \left[\beta_{s, mut}^{(OLS)} \right] = \frac{(\sigma_s^2 + \tau_s^2)(1 + n_{nonmut})}{n_{nonmut}}
\end{equation}

\section{Derivation of the James-Stein estimator}

For a $p$-variate normal distribution $Z \sim \mathcal{N}_p(\mu, \Sigma)$ where $\mu$ is unknown and $\Sigma$ is known, if we observe a single realization of this distribution, $z$, we have the following theorem \cite{steinInadmissibilityUsualEstimator1956}:

\begin{theorem}
  The estimator $\hat{\mu}^{(0)} = z$, for any mean $\mu$, does not minimize the \gls{mse} $\mathbb{E} \left[ (\mu - \hat{\mu})^2 \right]$ in the case that $m \ge 3$ and $\Sigma = I_N$. Namely, the estimator $\hat{\mu}^{(JS)} = \left( 1 - \frac{b}{a + \Vert z \Vert ^2}\right) z$ has a smaller \gls{mse} than $\hat{\mu}^{(0)}$ for sufficiently small $b$ and large $a$.
\end{theorem}

This result was generalized to non-singular covariance matrices that were not necessarily the identity matrix (Theorem 2 of \cite[REF][]{bockMinimaxEstimatorsMean1975}):

\begin{theorem}
  Let $\hat{\mu}^{(JS)} = \left( 1 - \frac{c}{z^T \Sigma^{-1} z}\right) z$. If $\text{Tr}(\Sigma) \ge 2 \lambda_L$ where $\lambda_L$ is the largest eigenvalue of the covariance matrix $\Sigma$ and $0 \le c \le 2 \left( \frac{\text{Tr}(\Sigma)}{\lambda_L} - 2 \right)$, then $\hat{\mu}^{(JS)}$ is the minimax estimator for the mean $\mu$.
\end{theorem}

Consider the Sleuth model with the simple experimental design above:

\begin{equation*}
  D_s | Y_s \sim \mathcal{N}_p \left( \beta_{s,0} + \mathbb{I}_{mut}\beta_{s,1}, (\sigma_s^2 + \tau_s^2)I_N \right)
\end{equation*}

For the $n_{nonmut}$ non-mutated samples, this is equivalent to

\begin{equation*}
  D_s | Y_s \sim \mathcal{N}_{n_{nonmut}} \left( \beta_{s,0}, (\sigma_s^2 + \tau_s^2)I_n \right)
\end{equation*}

which can be fit with the same model process that Sleuth employs.
For the single mutated sample, this model is

\begin{equation}
  D_s | Y_s \sim \mathcal{N} \left( \beta_{s, 0} + \beta_{s, 1}, \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right)
  \label{eqn:single_mut_model}
\end{equation}

The covariance matrix is the same as the mutated samples, but the mean $\beta_{s, 0} + \beta_{s, 1}$ is unknown and we have a single observation of this distribution.
Reparameterizing \Cref{eqn:single_mut_model} to consider every transcript in the single mutated sample can be written as follows:

\begin{equation}
  \Delta \sim \mathcal{N}_{|S|}(\Beta_0 + \Beta_1, \Sigma) \\
\end{equation}

where

\begin{align}
  \Beta_{i,s} &= \beta_{s,i} \forall s \in S \\
  \Sigma &= \begin{bmatrix}
    \max\{ \hat{\sigma}_1^2, \tilde{\sigma}_1^2 \} + \hat{\tau}_1^2 & & 0 \\
    & \ddots & \\
    0 & & \max\{ \hat{\sigma}_{|S|}^2, \tilde{\sigma}_{|S|}^2 \} + \hat{\tau}_{|S|}^2 \\
  \end{bmatrix}
\end{align}

We switch from using coefficients $\beta_{t,i}$ to $\Beta_{i,s}$ to avoid confusion, since $\beta_{t,i} \in \mathbb{R}^p$ (a $p$-dimensional vector for each covariate in the design) whereas $\Beta_{i,s} \in \mathbb{R}^{|S|}$ (an $|S|$-dimensional vector for only a single coefficient over all transcripts in $S$).

Observations of a single mutated sample from this model meet the criteria for the James-Stein estimators.
Let $\delta$ be a single observation of the distribution $\Delta$.
A James-Stein estimator for the unknown effect coefficient, $\Beta_1$, can be constructed.

\begin{equation}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{(\delta - \hat{\Beta}_0)^T \Sigma^{-1} (\delta - \hat{\Beta}_0)} \right)(\delta - \hat{\Beta}_0)
  \label{eqn:js_defn}
\end{equation}

where $\hat{\Beta}_0$ is the estimate obtained from the non-mutated samples for all transcripts $s \in S$.

It is simple to see that $\text{Tr}(\Sigma) = \sum_{s \in S} \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2$ and that $\lambda_L = \max_{s \in S} \left\{ \max\{ \hat{\sigma}_s^2, \tilde{\sigma}_s^2 \} + \hat{\tau}_s^2 \right\}$.

\section{Comparison between the \glsentryshort{ols} and James-Stein estimators}

For a simple experimental design where the mutation status is the only coefficient the \gls{ols} estimator is given by:

\begin{equation*}
  \begin{bmatrix}
    \hat{\beta}_{s,0}^{(OLS)} \\
    \hat{\beta}_{s,1}^{(OLS)}
  \end{bmatrix}
  = \hat{\beta}_s^{(OLS)}
  = (X^TX)^{-1}X^T d_s
  = \begin{bmatrix}
    \bar{d}_s^{(nonmut)} \\
    d_s^{(mut)} - \bar{d}_s^{(nonmut)}
  \end{bmatrix}
\end{equation*}

Looking closely at the \gls{ols} estimator for the mutation coefficient, $\beta_{s,1}$, it is clear that it is given by:

\begin{equation}
  \hat{\beta}_{s,1}^{(OLS)} = d_s^{(mut)} - \hat{\beta}_{s,0}^{(OLS)} = \delta_s - \hat{\beta}_{0,s}
\end{equation}

which is used directly in the definition of the James-Stein estimator in \Cref{eqn:js_defn}.
The James-Stein estimator for $\Beta_1$ can then be expressed simply as:

\begin{equation}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_1^{(OLS)} \right)^T \Sigma^{-1} \hat{\Beta}_1^{(OLS)}} \right) \hat{\Beta}_1^{(OLS)}
  \label{eqn:js_defn_ols}
\end{equation}

From this definition, it is easy to see that the James-Stein estimate is colinear with the \gls{ols} estimate but uniformly shrunk towards 0.

For a more general experimental design, the above can be extended.
Given an experimental design matrix

\begin{equation*}
  X \in \mathbb{R}^{n \times p}
\end{equation*}

where $n > p$, $\text{rank}(X) = p$ and $\text{rank}(X^*) = p - 1$ where $X^* \in \mathbb{R}^{(n - 1) \times p}$ is the same design matrix but with one sample removed, a James-Stein estimator for the linear coefficient uniquely specified by the one sample is given by

\begin{equation*}
  \hat{\Beta}_i^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_i^{(OLS)} \right)^T \Sigma^{-1} \hat{\Beta}_i^{(OLS)}} \right) \hat{\Beta}_i^{(OLS)}
\end{equation*}

\section{Statistical moments of the James-Stein fold change estimator}

\subsection{Expected value of the James-Stein estimator}

Due to the non-linear nature of the James-Stein estimator, a Taylor expansion around $\Beta_1$ can be used to approximate the expectation.
Consider:

\begin{equation*}
  \hat{\Beta}_1^{(JS)} = \left( 1 - \frac{c}{\left( \hat{\Beta}_1^{(OLS)} \right)^T \Sigma^{-1} \hat{\Beta}_1^{(OLS)}} \right) \hat{\Beta}_1^{(OLS)}
\end{equation*}

where

\begin{align*}
  \hat{\Beta}_1^{(OLS)} &\sim N_{|S|}(\Beta_1, \Sigma) \\
  \Sigma_{s,s} &= \left( \frac{n_{nonmut} + 1}{n_{nonmut}} \right) (\sigma_t^2 + \tau_t^2) \\
  \Sigma_{s,t} &= 0 \forall t \ne s
\end{align*}

Let $u = \Sigma^{-1/2}\hat{\Beta}_1^{(OLS)}$.
Then

\begin{align*}
  \mathbb{E}\left[ \hat{\Beta}_1^{(JS)} \right] &= \mathbb{E} \left[ \hat{\Beta}_1^{(OLS)} \right] - c\Sigma^{1/2}\mathbb{E} \left[ \frac{u}{\Vert u \Vert ^2} \right] \\
  &= \Beta_1 - c\Sigma^{1/2}\mathbb{E} \left[ \frac{u}{\Vert u \Vert ^2} \right] \Sigma^{1/2}\\
\end{align*}

Expanding $\frac{u}{\Vert u \Vert ^2}$ around $a = \Sigma^{-1/2}\Beta_1$ gives:

\begin{align*}
  \mathbb{E}\left[ \hat{\Beta}_1^{(JS)} \right]
  &= \Beta_1 - c\Sigma^{1/2}\mathbb{E} \left[ \frac{a}{\Vert a \Vert ^2} + \left( \frac{1}{\Vert a \Vert ^2}
    - \frac{2}{\Vert a \Vert ^4} aa^T\right)(u - a) + \mathcal{O}(\Vert u - a \Vert ^2)\right] \\
  & = \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Beta_1
    + \mathcal{O}(\Vert u - a \Vert ^2)
\end{align*}

As long as the number of transcripts being considered, $|S|$, is not large, and that the true coefficient of variation is not large (i.e. that $\Vert u - a \Vert ^2 \ll \Vert \Beta_1 \Vert ^2$), the Taylor approximation is close to

\begin{equation}
  \mathbb{E}\left[ \hat{\Beta}_1^{(JS)} \right] \approx \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Beta_1
\end{equation}

Thus the James-Stein estimator is an estimate of $\Beta_1$ that is biased towards 0.

\subsection{Variance of the James-Stein estimator}

The \gls{mse} of the James-Stein estimator is related to its variance.

\begin{equation*}
  \mathbb{E} \left[ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2\right] 
  = \sum_{s \in S} \mathbb{E}\left[ \left( \hat{\Beta}_{1,s}^{(JS)} - \Beta_{1,s} \right)^2 \right] \\
  = \sum_{s \in S} \mathbb{V}\left[ \hat{\Beta}_{1,s}^{(JS)} \right]
\end{equation*}

By \cite[REF][]{bockMinimaxEstimatorsMean1975}, $\mathbb{E} \left[ \Vert \hat{\Beta}_1^{(JS)} - \Beta_1 \Vert ^2\right] \le \mathbb{E} \left[ \Vert \hat{\Beta}_1^{(OLS)} - \Beta_1 \Vert ^2\right]$.
However, this does not imply that $\mathbb{V} \left[ \hat{\Beta}_{1,s}^{(JS)} \right] \le \mathbb{V} \left[ \hat{\Beta}_{1,s}^{(OLS)} \right] \forall s \in S$.
Some transcripts may have larger variances than the \gls{ols} estimator, but all transcripts in aggregate will have a smaller \gls{mse}.
This is still desirable if the goal is to find if there is an effect on any transcripts in the set $S$, instead of a particular one within the set.

To calculate the variance for each individual transcript, a similar approach with Taylor expansions can be used, as above.

\begin{align*}
  &\mathbb{V} \left[ \hat{\Beta}_1^{(JS)} \right] \\
  &\approx \mathbb{E} \left[ \hat{\Beta}_1^{(JS)}
    \left( \hat{\Beta}_1^{(JS)} \right)^T \right]
    - \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right)^2 \Beta_1 \Beta_1^T \\
  &= \Sigma^{1/2} \mathbb{E} \left[ u u^T - \frac{2c}{u^Tu}uu^T + \left( \frac{c}{u^Tu} \right)^2 uu^T \right]\Sigma^{1/2}
    - \left(1 - \frac{c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right)^2 \Beta_1 \Beta_1^T \\
\end{align*}

where, again, $u = \Sigma^{-1/2} \hat{\Beta}_1^{(OLS)}$.
Expanding about $a = \Sigma^{-1/2} \Beta_1$ yields:


\begin{equation*}
  \mathbb{V} \left [ \hat{\Beta}_1^{(JS)} \right]
  = \left(1 - \frac{2c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Sigma
    - \frac{2c}{\left( \Beta_1^T \Sigma^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^T
    + \mathcal{O}(\Vert u - a \Vert ^4)
\end{equation*}

Under similar conditions of the number of transcripts under consideration, $|S|$, and $\Vert u - a \Vert ^2$, we then have that

\begin{equation}
  \mathbb{V} \left [ \hat{\Beta}_1^{(JS)} \right]
  \approx \left(1 - \frac{2c}{\Beta_1^T \Sigma^{-1} \Beta_1} \right) \Sigma
  - \frac{2c}{\left( \Beta_1^T \Sigma^{-1} \Beta_1 \right)^2} \Beta_1 \Beta_1^T
\end{equation}

Again, a shrinkage factor multiplies $\Sigma$, providing the possibility of a smaller estimator variance than the \gls{ols} estimator.
This smaller variance allows for more powerful statistical inferences when using a Wald test.
Notably, $\Beta_1$ is unknown, so the variance of the James-Stein estimator is a function of both the mean and variance of the transcripts under consideration.
This is in contrast to the \gls{ols} estimator, which is solely a function of the variance.
